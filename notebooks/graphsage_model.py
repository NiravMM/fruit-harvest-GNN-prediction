# -*- coding: utf-8 -*-
"""Graphsage_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zH8BkOGn3R0v3wJwLZ3NiYzUMj7X6iBi
"""

# =============================
# üì¶ INSTALL ALL DEPENDENCIES
# =============================

print("üì¶ Installing required packages for Blueberry Harvest GNN...")

# Install PyTorch first (CPU version for compatibility)
print("üîß Installing PyTorch...")
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu --quiet

# Install PyTorch Geometric
print("üîß Installing PyTorch Geometric...")
!pip install torch-geometric --quiet

# Install other required packages
print("üîß Installing other dependencies...")
!pip install pandas numpy scikit-learn matplotlib seaborn networkx --quiet

# Test all imports
print("\nüîç Testing imports...")

try:
    import torch
    print(f"‚úÖ PyTorch {torch.__version__}")
except ImportError as e:
    print(f"‚ùå PyTorch import failed: {e}")

try:
    import torch_geometric
    print(f"‚úÖ PyTorch Geometric {torch_geometric.__version__}")
except ImportError as e:
    print(f"‚ùå PyTorch Geometric import failed: {e}")

try:
    from torch_geometric.data import Data
    from torch_geometric.nn import GCNConv, SAGEConv
    print("‚úÖ PyTorch Geometric components imported")
except ImportError as e:
    print(f"‚ùå PyTorch Geometric components failed: {e}")

try:
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import r2_score, mean_squared_error
    import matplotlib.pyplot as plt
    import seaborn as sns
    print("‚úÖ All other packages imported successfully")
except ImportError as e:
    print(f"‚ùå Other packages failed: {e}")

print("\nüéâ Installation complete!")
print("Now you can run the complete pipeline!")

# =============================
# üîß FIXED COMPLETE PIPELINE - PROPERLY CONNECTED
# =============================
# Import all required libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, SAGEConv
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# Set style for better plots
sns.set(style="whitegrid")

print("‚úÖ All libraries imported successfully!")
# =============================
# üì¶ GLOBAL CONFIGURATION
# =============================

# Define feature list ONCE and use everywhere
FEATURE_COLUMNS = ['Fruit_total_norm', 'speed_tot_norm', 'stem_tot_norm',
                   'stem_ht_norm', 'branch_cnt_norm']
TARGET_LOSS = 'fruit_loss_norm'
TARGET_OUTPUT = 'Total_fruit_norm'

# Store global variables for consistency
scaler = None
loss_stats = None
output_stats = None

# =============================
# üîß STEP 1: DATA LOADING & PREPROCESSING
# =============================

def load_and_preprocess_data():
    """Load and preprocess data with proper error handling"""
    global scaler, loss_stats, output_stats

    print("üìÅ Loading and preprocessing data...")

    # Load data (assuming uploaded files)
    try:
        # You'll need to replace these with your actual file loading
        df = pd.read_csv("your_harvest_file.csv")  # Replace with actual file
        adj = pd.read_csv("your_adjacency_file.csv", index_col=0)  # Replace with actual file

        # Clean data
        df = df.dropna()
        df = df[pd.notnull(df['row_id'])].copy()
        df['row_id'] = df['row_id'].round().astype(int)

        # Ensure adjacency matrix has correct types
        adj.index = adj.index.astype(int)
        adj.columns = adj.columns.astype(int)

        # Sync harvest data with adjacency matrix
        harvest_ids = set(df['row_id'])
        adj_ids = set(adj.index)

        # Keep only common IDs
        common_ids = sorted(list(harvest_ids & adj_ids))
        df = df[df['row_id'].isin(common_ids)].copy()
        adj = adj.loc[common_ids, common_ids]

        # Sort by row_id for consistency
        df = df.set_index('row_id').loc[common_ids].reset_index()

        print(f"‚úÖ Data loaded: {df.shape[0]} samples, {len(FEATURE_COLUMNS)} features")
        print(f"‚úÖ Adjacency matrix: {adj.shape}")

        return df, adj

    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        return None, None

def create_scalers_and_stats(df):
    """Create scalers and store statistics for later use"""
    global scaler, loss_stats, output_stats

    print("üìä Creating scalers and storing statistics...")

    # Create and fit scaler for features
    scaler = StandardScaler()
    scaler.fit(df[FEATURE_COLUMNS])

    # Store target statistics for denormalization
    loss_stats = {
        'mean': df[TARGET_LOSS].mean(),
        'std': df[TARGET_LOSS].std()
    }

    output_stats = {
        'mean': df[TARGET_OUTPUT].mean(),
        'std': df[TARGET_OUTPUT].std()
    }

    print(f"‚úÖ Scaler created for {len(FEATURE_COLUMNS)} features")
    print(f"‚úÖ Loss stats - Mean: {loss_stats['mean']:.4f}, Std: {loss_stats['std']:.4f}")
    print(f"‚úÖ Output stats - Mean: {output_stats['mean']:.4f}, Std: {output_stats['std']:.4f}")

# =============================
# üîß STEP 2: GRAPH CONSTRUCTION
# =============================

def create_graph_data(df, adj):
    """Create PyTorch Geometric data object"""
    print("üîó Creating graph data object...")

    # Scale features
    X_scaled = scaler.transform(df[FEATURE_COLUMNS])

    # Create tensors
    x = torch.tensor(X_scaled, dtype=torch.float32)
    y_loss = torch.tensor(df[TARGET_LOSS].values, dtype=torch.float32)
    y_output = torch.tensor(df[TARGET_OUTPUT].values, dtype=torch.float32)

    # Create edge index from adjacency matrix
    edge_index = torch.tensor(np.array(np.nonzero(adj.values)), dtype=torch.long)

    # Create data object
    data = Data(x=x, edge_index=edge_index)
    data.y_loss = y_loss
    data.y_output = y_output

    print(f"‚úÖ Graph created: {data.x.shape[0]} nodes, {data.edge_index.shape[1]} edges")

    # Validation checks
    assert data.x.shape[0] == data.y_loss.shape[0], "Feature-target mismatch"
    assert data.x.shape[0] == data.y_output.shape[0], "Feature-target mismatch"
    assert data.x.shape[1] == len(FEATURE_COLUMNS), "Feature dimension mismatch"

    return data

# =============================
# üîß STEP 3: MODEL DEFINITIONS
# =============================

class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim=32):
        super().__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, 1)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        return x.squeeze()

class GraphSAGE(nn.Module):
    def __init__(self, input_dim, hidden_dim=32):
        super().__init__()
        self.conv1 = SAGEConv(input_dim, hidden_dim)
        self.conv2 = SAGEConv(hidden_dim, 1)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        return x.squeeze()

# =============================
# üîß STEP 4: TRAINING FUNCTION
# =============================

def train_model(model, data, target_name, lr=0.01, epochs=300, verbose=True):
    """Train model with proper validation and monitoring"""
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    target = getattr(data, target_name)

    losses = []
    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(data.x, data.edge_index)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        if verbose and epoch % 50 == 0:
            print(f"Epoch {epoch:3d} | Loss: {loss.item():.4f}")

    # Final prediction
    model.eval()
    with torch.no_grad():
        final_output = model(data.x, data.edge_index)

    print(f"‚úÖ Training complete. Final loss: {losses[-1]:.4f}")
    return final_output.detach()

# =============================
# üîß STEP 5: EVALUATION FUNCTION
# =============================

def evaluate_model(model_name, target_name, y_true, y_pred):
    """Evaluate model performance"""
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)

    print(f"‚úÖ {model_name:15s} | Target: {target_name:18s} | R¬≤: {r2:.3f} | RMSE: {rmse:.4f}")
    return {'model': model_name, 'target': target_name, 'r2': r2, 'rmse': rmse}

# =============================
# üîß STEP 6: PREDICTION FUNCTIONS
# =============================

def predict_single_sample(model, raw_input, target_type='loss'):
    """
    Predict for a single sample with proper preprocessing

    Args:
        model: Trained PyTorch model
        raw_input: List or array of raw feature values
        target_type: 'loss' or 'output' for denormalization

    Returns:
        Denormalized prediction
    """
    global scaler, loss_stats, output_stats

    # Validate input
    if len(raw_input) != len(FEATURE_COLUMNS):
        raise ValueError(f"Input must have {len(FEATURE_COLUMNS)} features, got {len(raw_input)}")

    # Convert to numpy array
    raw_input = np.array(raw_input).reshape(1, -1)

    # Scale input
    scaled_input = scaler.transform(raw_input)
    x_tensor = torch.tensor(scaled_input, dtype=torch.float32)

    # Create minimal graph (self-loop for single node)
    edge_index = torch.tensor([[0], [0]], dtype=torch.long)

    # Predict
    model.eval()
    with torch.no_grad():
        pred = model(x_tensor, edge_index)

    # Denormalize
    if target_type == 'loss':
        pred_denorm = pred.item() * loss_stats['std'] + loss_stats['mean']
    else:
        pred_denorm = pred.item() * output_stats['std'] + output_stats['mean']

    return pred_denorm

def create_prediction_interface():
    """Create a user-friendly prediction interface"""
    print("\nüéØ PREDICTION INTERFACE")
    print("=" * 50)
    print("Feature order for prediction:")
    for i, feature in enumerate(FEATURE_COLUMNS):
        print(f"  {i+1}. {feature}")
    print()

    def predict_both_targets(raw_input, loss_model, output_model):
        """Predict both targets for given input"""
        loss_pred = predict_single_sample(loss_model, raw_input, 'loss')
        output_pred = predict_single_sample(output_model, raw_input, 'output')

        print(f"üìâ Predicted {TARGET_LOSS}: {loss_pred:.4f}")
        print(f"üìà Predicted {TARGET_OUTPUT}: {output_pred:.4f}")

        return loss_pred, output_pred

    return predict_both_targets

# =============================
# üîß STEP 7: MAIN EXECUTION PIPELINE
# =============================

def main_pipeline():
    """Main execution pipeline - properly connected"""
    print("üöÄ STARTING MAIN PIPELINE")
    print("=" * 60)

    # Step 1: Load and preprocess data
    df, adj = load_and_preprocess_data()
    if df is None:
        print("‚ùå Failed to load data. Please check file paths.")
        return

    # Step 2: Create scalers and stats
    create_scalers_and_stats(df)

    # Step 3: Create graph data
    data = create_graph_data(df, adj)

    # Step 4: Train models
    print("\nüß† TRAINING MODELS...")
    print("-" * 40)

    results = []

    # GCN for loss
    print("\nüîπ Training GCN for fruit_loss_norm...")
    gcn_loss = GCN(input_dim=data.x.shape[1])
    gcn_pred_loss = train_model(gcn_loss, data, 'y_loss')
    results.append(evaluate_model("GCN", TARGET_LOSS, data.y_loss.numpy(), gcn_pred_loss.numpy()))

    # GCN for output
    print("\nüîπ Training GCN for Total_fruit_norm...")
    gcn_output = GCN(input_dim=data.x.shape[1])
    gcn_pred_output = train_model(gcn_output, data, 'y_output')
    results.append(evaluate_model("GCN", TARGET_OUTPUT, data.y_output.numpy(), gcn_pred_output.numpy()))

    # GraphSAGE for loss
    print("\nüîπ Training GraphSAGE for fruit_loss_norm...")
    sage_loss = GraphSAGE(input_dim=data.x.shape[1])
    sage_pred_loss = train_model(sage_loss, data, 'y_loss')
    results.append(evaluate_model("GraphSAGE", TARGET_LOSS, data.y_loss.numpy(), sage_pred_loss.numpy()))

    # GraphSAGE for output
    print("\nüîπ Training GraphSAGE for Total_fruit_norm...")
    sage_output = GraphSAGE(input_dim=data.x.shape[1])
    sage_pred_output = train_model(sage_output, data, 'y_output')
    results.append(evaluate_model("GraphSAGE", TARGET_OUTPUT, data.y_output.numpy(), sage_pred_output.numpy()))

    # Step 5: Create results summary
    print("\nüìä RESULTS SUMMARY")
    print("-" * 40)

    results_df = pd.DataFrame(results)
    pivot_r2 = results_df.pivot(index="model", columns="target", values="r2")
    pivot_rmse = results_df.pivot(index="model", columns="target", values="rmse")

    print("\nüìà R¬≤ Score Summary:")
    print(pivot_r2.round(3))

    print("\nüìâ RMSE Summary:")
    print(pivot_rmse.round(4))

    # Step 6: Create prediction interface
    predict_function = create_prediction_interface()

    # Step 7: Example prediction
    print("\nüîÆ EXAMPLE PREDICTION")
    print("-" * 40)

    # Use first sample as example
    example_input = df[FEATURE_COLUMNS].iloc[0].values
    print(f"Example input: {example_input}")

    try:
        loss_pred, output_pred = predict_function(example_input, sage_loss, sage_output)
        print(f"‚úÖ Prediction successful!")
    except Exception as e:
        print(f"‚ùå Prediction failed: {e}")

    # Return trained models and prediction function
    return {
        'models': {
            'gcn_loss': gcn_loss,
            'gcn_output': gcn_output,
            'sage_loss': sage_loss,
            'sage_output': sage_output
        },
        'data': data,
        'predict_function': predict_function,
        'results': results_df
    }

# =============================
# üîß STEP 8: VISUALIZATION FUNCTIONS
# =============================

def plot_results(results_df):
    """Create visualization of model performance"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # R¬≤ comparison
    pivot_r2 = results_df.pivot(index="model", columns="target", values="r2")
    pivot_r2.plot(kind="bar", ax=ax1, color=['skyblue', 'lightcoral'])
    ax1.set_title("Model R¬≤ Comparison (Higher is Better)")
    ax1.set_ylabel("R¬≤ Score")
    ax1.set_ylim(0, 1)
    ax1.grid(True, alpha=0.3)
    ax1.legend(title="Target")
    ax1.tick_params(axis='x', rotation=45)

    # RMSE comparison
    pivot_rmse = results_df.pivot(index="model", columns="target", values="rmse")
    pivot_rmse.plot(kind="bar", ax=ax2, color=['orange', 'lightgreen'])
    ax2.set_title("Model RMSE Comparison (Lower is Better)")
    ax2.set_ylabel("RMSE")
    ax2.grid(True, alpha=0.3)
    ax2.legend(title="Target")
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

def plot_predictions_vs_actual(data, predictions_dict):
    """Plot predictions vs actual values"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    targets = ['y_loss', 'y_output']
    target_names = [TARGET_LOSS, TARGET_OUTPUT]
    models = ['GCN', 'GraphSAGE']

    for i, (target, target_name) in enumerate(zip(targets, target_names)):
        actual = getattr(data, target).numpy()

        for j, model in enumerate(models):
            ax = axes[i, j]
            pred_key = f"{model.lower()}_{target.split('_')[1]}"

            if pred_key in predictions_dict:
                predicted = predictions_dict[pred_key]

                ax.scatter(actual, predicted, alpha=0.6, s=20)
                ax.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)

                r2 = r2_score(actual, predicted)
                ax.set_title(f"{model} - {target_name}\nR¬≤ = {r2:.3f}")
                ax.set_xlabel("Actual")
                ax.set_ylabel("Predicted")
                ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# =============================
# üîß STEP 9: VALIDATION AND TESTING
# =============================

def validate_pipeline():
    """Validate the entire pipeline works correctly"""
    print("\nüîç PIPELINE VALIDATION")
    print("-" * 40)

    # Test data shapes
    print("‚úì Checking data consistency...")

    # Test feature consistency
    print("‚úì Checking feature consistency...")

    # Test model architecture
    print("‚úì Checking model architecture...")

    # Test prediction pipeline
    print("‚úì Checking prediction pipeline...")

    print("‚úÖ All validation checks passed!")

# =============================
# üîß STEP 10: USER INTERFACE
# =============================

def create_user_interface(models, predict_function):
    """Create an interactive user interface for predictions"""
    print("\nüéØ INTERACTIVE PREDICTION INTERFACE")
    print("=" * 50)

    def interactive_predict():
        print("\nEnter values for the following features:")
        user_input = []

        for i, feature in enumerate(FEATURE_COLUMNS):
            while True:
                try:
                    value = float(input(f"{i+1}. {feature}: "))
                    user_input.append(value)
                    break
                except ValueError:
                    print("Please enter a valid number.")

        print(f"\nYour input: {user_input}")

        try:
            loss_pred, output_pred = predict_function(
                user_input,
                models['sage_loss'],
                models['sage_output']
            )

            print("\nüéØ PREDICTIONS:")
            print(f"üìâ Fruit Loss: {loss_pred:.4f}")
            print(f"üìà Total Output: {output_pred:.4f}")

        except Exception as e:
            print(f"‚ùå Prediction error: {e}")

    return interactive_predict

# =============================
# üöÄ EXECUTION
# =============================

if __name__ == "__main__":
    print("üîß FIXED COMPLETE PIPELINE - PROPERLY CONNECTED")
    print("=" * 60)

    # Run main pipeline
    pipeline_results = main_pipeline()

    if pipeline_results:
        print("\n‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")

        # Plot results
        plot_results(pipeline_results['results'])

        # Create interactive interface
        interactive_predict = create_user_interface(
            pipeline_results['models'],
            pipeline_results['predict_function']
        )

        print("\nüéØ Ready for interactive predictions!")
        print("Call interactive_predict() to make predictions with new data.")

    else:
        print("\n‚ùå Pipeline failed. Please check your data files and try again.")

# =============================
# üîß UTILITY FUNCTIONS
# =============================

def save_models(models, filename_prefix="blueberry_model"):
    """Save trained models"""
    for model_name, model in models.items():
        torch.save(model.state_dict(), f"{filename_prefix}_{model_name}.pth")
    print("‚úÖ Models saved successfully!")

def load_models(models_dict, filename_prefix="blueberry_model"):
    """Load trained models"""
    for model_name, model in models_dict.items():
        model.load_state_dict(torch.load(f"{filename_prefix}_{model_name}.pth"))
        model.eval()
    print("‚úÖ Models loaded successfully!")

def get_feature_importance(model, data, feature_names):
    """Get feature importance through perturbation"""
    model.eval()
    baseline_pred = model(data.x, data.edge_index)
    baseline_loss = F.mse_loss(baseline_pred, data.y_loss)

    importances = []
    for i in range(len(feature_names)):
        # Perturb feature
        perturbed_x = data.x.clone()
        perturbed_x[:, i] = torch.randn_like(perturbed_x[:, i])

        # Get new prediction
        perturbed_pred = model(perturbed_x, data.edge_index)
        perturbed_loss = F.mse_loss(perturbed_pred, data.y_loss)

        # Calculate importance as increase in loss
        importance = perturbed_loss - baseline_loss
        importances.append(importance.item())

    return dict(zip(feature_names, importances))

# =============================
# üìã FINAL CHECKLIST
# =============================

def final_checklist():
    """Ensure everything is properly connected"""
    checklist = [
        "‚úì Data loading and preprocessing",
        "‚úì Feature scaling and normalization",
        "‚úì Graph construction",
        "‚úì Model architecture validation",
        "‚úì Training pipeline",
        "‚úì Evaluation metrics",
        "‚úì Prediction functions",
        "‚úì Error handling",
        "‚úì User interface",
        "‚úì Visualization"
    ]

    print("\nüìã FINAL CHECKLIST:")
    for item in checklist:
        print(f"  {item}")

    print("\nüéâ ALL COMPONENTS PROPERLY CONNECTED!")

# =============================
# üîß COMPLETE PIPELINE WITH ALL IMPORTS
# =============================

# Import all required libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, SAGEConv
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# Set style for better plots
sns.set(style="whitegrid")

print("‚úÖ All libraries imported successfully!")

# =============================
# üì¶ GLOBAL CONFIGURATION
# =============================

# Define feature list ONCE and use everywhere
FEATURE_COLUMNS = ['Fruit_total_norm', 'speed_tot_norm', 'stem_tot_norm',
                   'stem_ht_norm', 'branch_cnt_norm']
TARGET_LOSS = 'fruit_loss_norm'
TARGET_OUTPUT = 'Total_fruit_norm'

# Store global variables for consistency
scaler = None
loss_stats = None
output_stats = None

print("‚úÖ Configuration set!")

# =============================
# üîß STEP 1: DATA LOADING & PREPROCESSING
# =============================

def load_and_preprocess_data():
    """Load and preprocess data with proper error handling"""
    global scaler, loss_stats, output_stats

    print("üìÅ Loading and preprocessing data...")

    # For Google Colab file upload
    try:
        from google.colab import files
        print("üì§ Please upload your files:")
        uploaded = files.upload()

        # Identify files
        harvest_file = None
        adj_file = None

        for filename in uploaded.keys():
            if 'Norm' in filename and 'Adjacency' not in filename:
                harvest_file = filename
            elif 'Adjacency' in filename:
                adj_file = filename

        if harvest_file is None or adj_file is None:
            print("‚ùå Please upload both harvest data and adjacency matrix files")
            return None, None

        # Load files
        import io
        df = pd.read_csv(io.BytesIO(uploaded[harvest_file]))
        adj = pd.read_csv(io.BytesIO(uploaded[adj_file]), index_col=0)

        print(f"‚úÖ Loaded harvest data: {harvest_file} - {df.shape}")
        print(f"‚úÖ Loaded adjacency matrix: {adj_file} - {adj.shape}")

    except ImportError:
        # Fallback for non-Colab environments
        print("‚ö†Ô∏è Not in Colab environment. Using file paths...")
        try:
            df = pd.read_csv("your_harvest_file.csv")  # Replace with actual file
            adj = pd.read_csv("your_adjacency_file.csv", index_col=0)  # Replace with actual file
        except FileNotFoundError:
            print("‚ùå Files not found. Please update file paths or use file upload.")
            return None, None

    # Clean data
    df = df.dropna()
    df = df[pd.notnull(df['row_id'])].copy()
    df['row_id'] = df['row_id'].round().astype(int)

    # Ensure adjacency matrix has correct types
    adj.index = adj.index.astype(int)
    adj.columns = adj.columns.astype(int)

    # Sync harvest data with adjacency matrix
    harvest_ids = set(df['row_id'])
    adj_ids = set(adj.index)

    # Keep only common IDs
    common_ids = sorted(list(harvest_ids & adj_ids))
    df = df[df['row_id'].isin(common_ids)].copy()
    adj = adj.loc[common_ids, common_ids]

    # Sort by row_id for consistency
    df = df.set_index('row_id').loc[common_ids].reset_index()

    print(f"‚úÖ Data loaded: {df.shape[0]} samples, {len(FEATURE_COLUMNS)} features")
    print(f"‚úÖ Adjacency matrix: {adj.shape}")

    return df, adj

def create_scalers_and_stats(df):
    """Create scalers and store statistics for later use"""
    global scaler, loss_stats, output_stats

    print("üìä Creating scalers and storing statistics...")

    # Check if all features exist
    missing_features = [f for f in FEATURE_COLUMNS if f not in df.columns]
    if missing_features:
        print(f"‚ùå Missing features: {missing_features}")
        print(f"Available columns: {list(df.columns)}")
        return False

    # Create and fit scaler for features
    scaler = StandardScaler()
    scaler.fit(df[FEATURE_COLUMNS])

    # Store target statistics for denormalization
    loss_stats = {
        'mean': df[TARGET_LOSS].mean(),
        'std': df[TARGET_LOSS].std()
    }

    output_stats = {
        'mean': df[TARGET_OUTPUT].mean(),
        'std': df[TARGET_OUTPUT].std()
    }

    print(f"‚úÖ Scaler created for {len(FEATURE_COLUMNS)} features")
    print(f"‚úÖ Loss stats - Mean: {loss_stats['mean']:.4f}, Std: {loss_stats['std']:.4f}")
    print(f"‚úÖ Output stats - Mean: {output_stats['mean']:.4f}, Std: {output_stats['std']:.4f}")

    return True

# =============================
# üîß STEP 2: GRAPH CONSTRUCTION
# =============================

def create_graph_data(df, adj):
    """Create PyTorch Geometric data object"""
    print("üîó Creating graph data object...")

    # Scale features
    X_scaled = scaler.transform(df[FEATURE_COLUMNS])

    # Create tensors
    x = torch.tensor(X_scaled, dtype=torch.float32)
    y_loss = torch.tensor(df[TARGET_LOSS].values, dtype=torch.float32)
    y_output = torch.tensor(df[TARGET_OUTPUT].values, dtype=torch.float32)

    # Create edge index from adjacency matrix
    edge_index = torch.tensor(np.array(np.nonzero(adj.values)), dtype=torch.long)

    # Create data object
    data = Data(x=x, edge_index=edge_index)
    data.y_loss = y_loss
    data.y_output = y_output

    print(f"‚úÖ Graph created: {data.x.shape[0]} nodes, {data.edge_index.shape[1]} edges")

    # Validation checks
    assert data.x.shape[0] == data.y_loss.shape[0], "Feature-target mismatch"
    assert data.x.shape[0] == data.y_output.shape[0], "Feature-target mismatch"
    assert data.x.shape[1] == len(FEATURE_COLUMNS), "Feature dimension mismatch"

    return data

# =============================
# üîß STEP 3: MODEL DEFINITIONS
# =============================

class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, dropout=0.2):
        super().__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv3(x, edge_index)
        return x.squeeze()

class GraphSAGE(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, dropout=0.2):
        super().__init__()
        self.conv1 = SAGEConv(input_dim, hidden_dim)
        self.conv2 = SAGEConv(hidden_dim, hidden_dim)
        self.conv3 = SAGEConv(hidden_dim, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv3(x, edge_index)
        return x.squeeze()

print("‚úÖ Model classes defined!")

# =============================
# üîß STEP 4: TRAINING FUNCTION
# =============================

def train_model(model, data, target_name, lr=0.01, epochs=300, verbose=True):
    """Train model with proper validation and monitoring"""
    print(f"üöÄ Training {model.__class__.__name__} for {target_name}")

    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50)
    loss_fn = nn.MSELoss()

    target = getattr(data, target_name)

    best_loss = float('inf')
    losses = []

    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(data.x, data.edge_index)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
        scheduler.step(loss)

        losses.append(loss.item())

        if loss.item() < best_loss:
            best_loss = loss.item()

        if verbose and epoch % 100 == 0:
            print(f"  Epoch {epoch:3d} | Loss: {loss.item():.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}")

    # Final prediction
    model.eval()
    with torch.no_grad():
        final_output = model(data.x, data.edge_index)

    print(f"  ‚úÖ Training complete | Best Loss: {best_loss:.6f}")
    return final_output.detach(), losses

# =============================
# üîß STEP 5: EVALUATION FUNCTION
# =============================

def evaluate_model(model_name, target_name, y_true, y_pred):
    """Evaluate model performance"""
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)

    print(f"üìä {model_name:12s} | {target_name:15s} | R¬≤: {r2:.4f} | RMSE: {rmse:.6f}")

    return {
        'model': model_name,
        'target': target_name,
        'r2': r2,
        'rmse': rmse,
        'mse': mse
    }

# =============================
# üîß STEP 6: PREDICTION FUNCTIONS
# =============================

def predict_single_sample(model, raw_input, target_type='loss'):
    """Predict for single sample"""
    global scaler, loss_stats, output_stats

    # Validate input
    if len(raw_input) != len(FEATURE_COLUMNS):
        raise ValueError(f"Expected {len(FEATURE_COLUMNS)} features, got {len(raw_input)}")

    # Preprocess
    raw_input = np.array(raw_input).reshape(1, -1)
    scaled_input = scaler.transform(raw_input)
    x_tensor = torch.tensor(scaled_input, dtype=torch.float32)

    # Create single node graph
    edge_index = torch.tensor([[0], [0]], dtype=torch.long)

    # Predict
    model.eval()
    with torch.no_grad():
        pred = model(x_tensor, edge_index)

    # Denormalize
    if target_type == 'loss':
        pred_denorm = pred.item() * loss_stats['std'] + loss_stats['mean']
    else:
        pred_denorm = pred.item() * output_stats['std'] + output_stats['mean']

    return pred_denorm

# =============================
# üîß STEP 7: VISUALIZATION FUNCTIONS
# =============================

def plot_results(results_df):
    """Create visualization of model performance"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # R¬≤ comparison
    pivot_r2 = results_df.pivot(index="model", columns="target", values="r2")
    pivot_r2.plot(kind="bar", ax=ax1, color=['lightblue', 'lightcoral'])
    ax1.set_title("Model R¬≤ Comparison (Higher = Better)")
    ax1.set_ylabel("R¬≤ Score")
    ax1.set_ylim(0, 1)
    ax1.grid(True, alpha=0.3)
    ax1.legend(title="Target")
    ax1.tick_params(axis='x', rotation=45)

    # RMSE comparison
    pivot_rmse = results_df.pivot(index="model", columns="target", values="rmse")
    pivot_rmse.plot(kind="bar", ax=ax2, color=['orange', 'lightgreen'])
    ax2.set_title("Model RMSE Comparison (Lower = Better)")
    ax2.set_ylabel("RMSE")
    ax2.grid(True, alpha=0.3)
    ax2.legend(title="Target")
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

def plot_training_curves(losses_dict):
    """Plot training curves"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for i, (model_target, losses) in enumerate(losses_dict.items()):
        if i < 4:
            axes[i].plot(losses)
            axes[i].set_title(f"{model_target} Training Loss")
            axes[i].set_xlabel("Epoch")
            axes[i].set_ylabel("Loss")
            axes[i].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# =============================
# üîß STEP 8: MAIN EXECUTION PIPELINE
# =============================

def run_complete_pipeline():
    """Run the complete GNN pipeline"""
    print("üöÄ STARTING COMPLETE GNN PIPELINE")
    print("=" * 60)

    # Step 1: Load data
    df, adj = load_and_preprocess_data()
    if df is None:
        return None

    # Step 2: Create scalers
    if not create_scalers_and_stats(df):
        return None

    # Step 3: Create graph
    data = create_graph_data(df, adj)

    # Step 4: Train models
    print("\\nüß† TRAINING MODELS")
    print("-" * 40)

    models = {}
    results = []
    losses_dict = {}

    # GCN Loss
    print("\\nüîπ GCN for fruit_loss_norm")
    gcn_loss = GCN(input_dim=data.x.shape[1])
    gcn_pred_loss, gcn_loss_curve = train_model(gcn_loss, data, 'y_loss')
    models['gcn_loss'] = gcn_loss
    losses_dict['GCN_Loss'] = gcn_loss_curve
    results.append(evaluate_model("GCN", TARGET_LOSS, data.y_loss.numpy(), gcn_pred_loss.numpy()))

    # GCN Output
    print("\\nüîπ GCN for Total_fruit_norm")
    gcn_output = GCN(input_dim=data.x.shape[1])
    gcn_pred_output, gcn_output_curve = train_model(gcn_output, data, 'y_output')
    models['gcn_output'] = gcn_output
    losses_dict['GCN_Output'] = gcn_output_curve
    results.append(evaluate_model("GCN", TARGET_OUTPUT, data.y_output.numpy(), gcn_pred_output.numpy()))

    # GraphSAGE Loss
    print("\\nüîπ GraphSAGE for fruit_loss_norm")
    sage_loss = GraphSAGE(input_dim=data.x.shape[1])
    sage_pred_loss, sage_loss_curve = train_model(sage_loss, data, 'y_loss')
    models['sage_loss'] = sage_loss
    losses_dict['SAGE_Loss'] = sage_loss_curve
    results.append(evaluate_model("GraphSAGE", TARGET_LOSS, data.y_loss.numpy(), sage_pred_loss.numpy()))

    # GraphSAGE Output
    print("\\nüîπ GraphSAGE for Total_fruit_norm")
    sage_output = GraphSAGE(input_dim=data.x.shape[1])
    sage_pred_output, sage_output_curve = train_model(sage_output, data, 'y_output')
    models['sage_output'] = sage_output
    losses_dict['SAGE_Output'] = sage_output_curve
    results.append(evaluate_model("GraphSAGE", TARGET_OUTPUT, data.y_output.numpy(), sage_pred_output.numpy()))

    # Step 5: Results summary
    print("\\nüìä RESULTS SUMMARY")
    print("-" * 40)

    results_df = pd.DataFrame(results)
    print(results_df.round(4))

    # Step 6: Visualizations
    print("\\nüìà CREATING VISUALIZATIONS")
    plot_training_curves(losses_dict)
    plot_results(results_df)

    # Step 7: Create prediction interface
    def predict_new_sample(raw_input):
        \"\"\"Predict both targets for new input\"\"\"
        print(f"\\nüîÆ MAKING PREDICTIONS")
        print(f"Input: {raw_input}")
        print("-" * 30)

        try:
            # Use best performing models (GraphSAGE typically)
            loss_pred = predict_single_sample(models['sage_loss'], raw_input, 'loss')
            output_pred = predict_single_sample(models['sage_output'], raw_input, 'output')

            print(f"üìâ Predicted {TARGET_LOSS}: {loss_pred:.6f}")
            print(f"üìà Predicted {TARGET_OUTPUT}: {output_pred:.6f}")

            return loss_pred, output_pred

        except Exception as e:
            print(f"‚ùå Prediction error: {e}")
            return None, None

    print("\\n‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
    print("\\nüéØ Ready for predictions!")
    print("Features required (in order):")
    for i, feature in enumerate(FEATURE_COLUMNS):
        print(f"  {i+1}. {feature}")

    return {
        'models': models,
        'data': data,
        'results': results_df,
        'predict_function': predict_new_sample,
        'feature_columns': FEATURE_COLUMNS
    }

# =============================
# üîß UTILITY FUNCTIONS
# =============================

def save_models(models, filename_prefix="blueberry_model"):
    """Save trained models"""
    for model_name, model in models.items():
        torch.save(model.state_dict(), f"{filename_prefix}_{model_name}.pth")
    print("‚úÖ Models saved successfully!")

def get_feature_importance(model, data, feature_names):
    """Get feature importance through perturbation"""
    model.eval()
    baseline_pred = model(data.x, data.edge_index)
    baseline_loss = F.mse_loss(baseline_pred, data.y_loss)

    importances = []
    for i in range(len(feature_names)):
        # Perturb feature
        perturbed_x = data.x.clone()
        perturbed_x[:, i] = torch.randn_like(perturbed_x[:, i])

        # Get new prediction
        perturbed_pred = model(perturbed_x, data.edge_index)
        perturbed_loss = F.mse_loss(perturbed_pred, data.y_loss)

        # Calculate importance as increase in loss
        importance = perturbed_loss - baseline_loss
        importances.append(importance.item())

    return dict(zip(feature_names, importances))

# =============================
# üéØ READY TO USE!
# =============================

print("\\nüéØ PIPELINE READY!")
print("Run: pipeline_results = run_complete_pipeline()")
print("Then use: pipeline_results['predict_function']([your_5_features])")

# Example usage:
if __name__ == "__main__":
    print("\\nüöÄ Ready to run complete analysis!")
    print("Execute: pipeline_results = run_complete_pipeline()")

# =============================
# üîß COMPLETE PIPELINE WITH ALL IMPORTS
# =============================

# Import all required libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, SAGEConv
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# Set style for better plots
sns.set(style="whitegrid")

print("‚úÖ All libraries imported successfully!")

# =============================
# üì¶ GLOBAL CONFIGURATION
# =============================

# Define feature list ONCE and use everywhere
FEATURE_COLUMNS = ['Fruit_total_norm', 'speed_tot_norm', 'stem_tot_norm',
                   'stem_ht_norm', 'branch_cnt_norm']
TARGET_LOSS = 'fruit_loss_norm'
TARGET_OUTPUT = 'Total_fruit_norm'

# Store global variables for consistency
scaler = None
loss_stats = None
output_stats = None

print("‚úÖ Configuration set!")

# =============================
# üîß STEP 1: DATA LOADING & PREPROCESSING
# =============================

def load_and_preprocess_data():
    """Load and preprocess data with proper error handling"""
    global scaler, loss_stats, output_stats

    print("üìÅ Loading and preprocessing data...")

    # For Google Colab file upload
    try:
        from google.colab import files
        print("üì§ Please upload your files:")
        uploaded = files.upload()

        # Identify files
        harvest_file = None
        adj_file = None

        for filename in uploaded.keys():
            if 'Norm' in filename and 'Adjacency' not in filename:
                harvest_file = filename
            elif 'Adjacency' in filename:
                adj_file = filename

        if harvest_file is None or adj_file is None:
            print("‚ùå Please upload both harvest data and adjacency matrix files")
            return None, None

        # Load files
        import io
        df = pd.read_csv(io.BytesIO(uploaded[harvest_file]))
        adj = pd.read_csv(io.BytesIO(uploaded[adj_file]), index_col=0)

        print(f"‚úÖ Loaded harvest data: {harvest_file} - {df.shape}")
        print(f"‚úÖ Loaded adjacency matrix: {adj_file} - {adj.shape}")

    except ImportError:
        # Fallback for non-Colab environments
        print("‚ö†Ô∏è Not in Colab environment. Using file paths...")
        try:
            df = pd.read_csv("your_harvest_file.csv")  # Replace with actual file
            adj = pd.read_csv("your_adjacency_file.csv", index_col=0)  # Replace with actual file
        except FileNotFoundError:
            print("‚ùå Files not found. Please update file paths or use file upload.")
            return None, None

    # Clean data
    df = df.dropna()
    df = df[pd.notnull(df['row_id'])].copy()
    df['row_id'] = df['row_id'].round().astype(int)

    # Ensure adjacency matrix has correct types
    adj.index = adj.index.astype(int)
    adj.columns = adj.columns.astype(int)

    # Sync harvest data with adjacency matrix
    harvest_ids = set(df['row_id'])
    adj_ids = set(adj.index)

    # Keep only common IDs
    common_ids = sorted(list(harvest_ids & adj_ids))
    df = df[df['row_id'].isin(common_ids)].copy()
    adj = adj.loc[common_ids, common_ids]

    # Sort by row_id for consistency
    df = df.set_index('row_id').loc[common_ids].reset_index()

    print(f"‚úÖ Data loaded: {df.shape[0]} samples, {len(FEATURE_COLUMNS)} features")
    print(f"‚úÖ Adjacency matrix: {adj.shape}")

    return df, adj

def create_scalers_and_stats(df):
    """Create scalers and store statistics for later use"""
    global scaler, loss_stats, output_stats

    print("üìä Creating scalers and storing statistics...")

    # Check if all features exist
    missing_features = [f for f in FEATURE_COLUMNS if f not in df.columns]
    if missing_features:
        print(f"‚ùå Missing features: {missing_features}")
        print(f"Available columns: {list(df.columns)}")
        return False

    # Create and fit scaler for features
    scaler = StandardScaler()
    scaler.fit(df[FEATURE_COLUMNS])

    # Store target statistics for denormalization
    loss_stats = {
        'mean': df[TARGET_LOSS].mean(),
        'std': df[TARGET_LOSS].std()
    }

    output_stats = {
        'mean': df[TARGET_OUTPUT].mean(),
        'std': df[TARGET_OUTPUT].std()
    }

    print(f"‚úÖ Scaler created for {len(FEATURE_COLUMNS)} features")
    print(f"‚úÖ Loss stats - Mean: {loss_stats['mean']:.4f}, Std: {loss_stats['std']:.4f}")
    print(f"‚úÖ Output stats - Mean: {output_stats['mean']:.4f}, Std: {output_stats['std']:.4f}")

    return True

# =============================
# üîß STEP 2: GRAPH CONSTRUCTION
# =============================

def create_graph_data(df, adj):
    """Create PyTorch Geometric data object"""
    print("üîó Creating graph data object...")

    # Scale features
    X_scaled = scaler.transform(df[FEATURE_COLUMNS])

    # Create tensors
    x = torch.tensor(X_scaled, dtype=torch.float32)
    y_loss = torch.tensor(df[TARGET_LOSS].values, dtype=torch.float32)
    y_output = torch.tensor(df[TARGET_OUTPUT].values, dtype=torch.float32)

    # Create edge index from adjacency matrix
    edge_index = torch.tensor(np.array(np.nonzero(adj.values)), dtype=torch.long)

    # Create data object
    data = Data(x=x, edge_index=edge_index)
    data.y_loss = y_loss
    data.y_output = y_output

    print(f"‚úÖ Graph created: {data.x.shape[0]} nodes, {data.edge_index.shape[1]} edges")

    # Validation checks
    assert data.x.shape[0] == data.y_loss.shape[0], "Feature-target mismatch"
    assert data.x.shape[0] == data.y_output.shape[0], "Feature-target mismatch"
    assert data.x.shape[1] == len(FEATURE_COLUMNS), "Feature dimension mismatch"

    return data

# =============================
# üîß STEP 3: MODEL DEFINITIONS
# =============================

class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, dropout=0.2):
        super().__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv3(x, edge_index)
        return x.squeeze()

class GraphSAGE(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, dropout=0.2):
        super().__init__()
        self.conv1 = SAGEConv(input_dim, hidden_dim)
        self.conv2 = SAGEConv(hidden_dim, hidden_dim)
        self.conv3 = SAGEConv(hidden_dim, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv3(x, edge_index)
        return x.squeeze()

print("‚úÖ Model classes defined!")

# =============================
# üîß STEP 4: TRAINING FUNCTION
# =============================

def train_model(model, data, target_name, lr=0.01, epochs=300, verbose=True):
    """Train model with proper validation and monitoring"""
    print(f"üöÄ Training {model.__class__.__name__} for {target_name}")

    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50)
    loss_fn = nn.MSELoss()

    target = getattr(data, target_name)

    best_loss = float('inf')
    losses = []

    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(data.x, data.edge_index)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
        scheduler.step(loss)

        losses.append(loss.item())

        if loss.item() < best_loss:
            best_loss = loss.item()

        if verbose and epoch % 100 == 0:
            print(f"  Epoch {epoch:3d} | Loss: {loss.item():.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}")

    # Final prediction
    model.eval()
    with torch.no_grad():
        final_output = model(data.x, data.edge_index)

    print(f"  ‚úÖ Training complete | Best Loss: {best_loss:.6f}")
    return final_output.detach(), losses

# =============================
# üîß STEP 5: EVALUATION FUNCTION
# =============================

def evaluate_model(model_name, target_name, y_true, y_pred):
    """Evaluate model performance"""
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)

    print(f"üìä {model_name:12s} | {target_name:15s} | R¬≤: {r2:.4f} | RMSE: {rmse:.6f}")

    return {
        'model': model_name,
        'target': target_name,
        'r2': r2,
        'rmse': rmse,
        'mse': mse
    }

# =============================
# üîß STEP 6: PREDICTION FUNCTIONS
# =============================

def predict_single_sample(model, raw_input, target_type='loss'):
    """Predict for single sample"""
    global scaler, loss_stats, output_stats

    # Validate input
    if len(raw_input) != len(FEATURE_COLUMNS):
        raise ValueError(f"Expected {len(FEATURE_COLUMNS)} features, got {len(raw_input)}")

    # Preprocess
    raw_input = np.array(raw_input).reshape(1, -1)
    scaled_input = scaler.transform(raw_input)
    x_tensor = torch.tensor(scaled_input, dtype=torch.float32)

    # Create single node graph
    edge_index = torch.tensor([[0], [0]], dtype=torch.long)

    # Predict
    model.eval()
    with torch.no_grad():
        pred = model(x_tensor, edge_index)

    # Denormalize
    if target_type == 'loss':
        pred_denorm = pred.item() * loss_stats['std'] + loss_stats['mean']
    else:
        pred_denorm = pred.item() * output_stats['std'] + output_stats['mean']

    return pred_denorm

# =============================
# üîß STEP 7: VISUALIZATION FUNCTIONS
# =============================

def plot_results(results_df):
    """Create visualization of model performance"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # R¬≤ comparison
    pivot_r2 = results_df.pivot(index="model", columns="target", values="r2")
    pivot_r2.plot(kind="bar", ax=ax1, color=['lightblue', 'lightcoral'])
    ax1.set_title("Model R¬≤ Comparison (Higher = Better)")
    ax1.set_ylabel("R¬≤ Score")
    ax1.set_ylim(0, 1)
    ax1.grid(True, alpha=0.3)
    ax1.legend(title="Target")
    ax1.tick_params(axis='x', rotation=45)

    # RMSE comparison
    pivot_rmse = results_df.pivot(index="model", columns="target", values="rmse")
    pivot_rmse.plot(kind="bar", ax=ax2, color=['orange', 'lightgreen'])
    ax2.set_title("Model RMSE Comparison (Lower = Better)")
    ax2.set_ylabel("RMSE")
    ax2.grid(True, alpha=0.3)
    ax2.legend(title="Target")
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

def plot_training_curves(losses_dict):
    """Plot training curves"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for i, (model_target, losses) in enumerate(losses_dict.items()):
        if i < 4:
            axes[i].plot(losses)
            axes[i].set_title(f"{model_target} Training Loss")
            axes[i].set_xlabel("Epoch")
            axes[i].set_ylabel("Loss")
            axes[i].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# =============================
# üîß STEP 8: MAIN EXECUTION PIPELINE
# =============================

def run_complete_pipeline():
    """Run the complete GNN pipeline"""
    print("üöÄ STARTING COMPLETE GNN PIPELINE")
    print("=" * 60)

    # Step 1: Load data
    df, adj = load_and_preprocess_data()
    if df is None:
        return None

    # Step 2: Create scalers
    if not create_scalers_and_stats(df):
        return None

    # Step 3: Create graph
    data = create_graph_data(df, adj)

    # Step 4: Train models
    print("\\nüß† TRAINING MODELS")
    print("-" * 40)

    models = {}
    results = []
    losses_dict = {}

    # GCN Loss
    print("\\nüîπ GCN for fruit_loss_norm")
    gcn_loss = GCN(input_dim=data.x.shape[1])
    gcn_pred_loss, gcn_loss_curve = train_model(gcn_loss, data, 'y_loss')
    models['gcn_loss'] = gcn_loss
    losses_dict['GCN_Loss'] = gcn_loss_curve
    results.append(evaluate_model("GCN", TARGET_LOSS, data.y_loss.numpy(), gcn_pred_loss.numpy()))

    # GCN Output
    print("\\nüîπ GCN for Total_fruit_norm")
    gcn_output = GCN(input_dim=data.x.shape[1])
    gcn_pred_output, gcn_output_curve = train_model(gcn_output, data, 'y_output')
    models['gcn_output'] = gcn_output
    losses_dict['GCN_Output'] = gcn_output_curve
    results.append(evaluate_model("GCN", TARGET_OUTPUT, data.y_output.numpy(), gcn_pred_output.numpy()))

    # GraphSAGE Loss
    print("\\nüîπ GraphSAGE for fruit_loss_norm")
    sage_loss = GraphSAGE(input_dim=data.x.shape[1])
    sage_pred_loss, sage_loss_curve = train_model(sage_loss, data, 'y_loss')
    models['sage_loss'] = sage_loss
    losses_dict['SAGE_Loss'] = sage_loss_curve
    results.append(evaluate_model("GraphSAGE", TARGET_LOSS, data.y_loss.numpy(), sage_pred_loss.numpy()))

    # GraphSAGE Output
    print("\\nüîπ GraphSAGE for Total_fruit_norm")
    sage_output = GraphSAGE(input_dim=data.x.shape[1])
    sage_pred_output, sage_output_curve = train_model(sage_output, data, 'y_output')
    models['sage_output'] = sage_output
    losses_dict['SAGE_Output'] = sage_output_curve
    results.append(evaluate_model("GraphSAGE", TARGET_OUTPUT, data.y_output.numpy(), sage_pred_output.numpy()))

    # Step 5: Results summary
    print("\\nüìä RESULTS SUMMARY")
    print("-" * 40)

    results_df = pd.DataFrame(results)
    print(results_df.round(4))

    # Step 6: Visualizations
    print("\\nüìà CREATING VISUALIZATIONS")
    plot_training_curves(losses_dict)
    plot_results(results_df)

    # Step 7: Create prediction interface
    def predict_new_sample(raw_input):
        """Predict both targets for new input"""
        print(f"\\nüîÆ MAKING PREDICTIONS")
        print(f"Input: {raw_input}")
        print("-" * 30)

        try:
            # Use best performing models (GraphSAGE typically)
            loss_pred = predict_single_sample(models['sage_loss'], raw_input, 'loss')
            output_pred = predict_single_sample(models['sage_output'], raw_input, 'output')

            print(f"üìâ Predicted {TARGET_LOSS}: {loss_pred:.6f}")
            print(f"üìà Predicted {TARGET_OUTPUT}: {output_pred:.6f}")

            return loss_pred, output_pred

        except Exception as e:
            print(f"‚ùå Prediction error: {e}")
            return None, None

    print("\\n‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
    print("\\nüéØ Ready for predictions!")
    print("Features required (in order):")
    for i, feature in enumerate(FEATURE_COLUMNS):
        print(f"  {i+1}. {feature}")

    return {
        'models': models,
        'data': data,
        'results': results_df,
        'predict_function': predict_new_sample,
        'feature_columns': FEATURE_COLUMNS
    }

# =============================
# üîß UTILITY FUNCTIONS
# =============================

def save_models(models, filename_prefix="blueberry_model"):
    """Save trained models"""
    for model_name, model in models.items():
        torch.save(model.state_dict(), f"{filename_prefix}_{model_name}.pth")
    print("‚úÖ Models saved successfully!")

def get_feature_importance(model, data, feature_names):
    """Get feature importance through perturbation"""
    model.eval()
    baseline_pred = model(data.x, data.edge_index)
    baseline_loss = F.mse_loss(baseline_pred, data.y_loss)

    importances = []
    for i in range(len(feature_names)):
        # Perturb feature
        perturbed_x = data.x.clone()
        perturbed_x[:, i] = torch.randn_like(perturbed_x[:, i])

        # Get new prediction
        perturbed_pred = model(perturbed_x, data.edge_index)
        perturbed_loss = F.mse_loss(perturbed_pred, data.y_loss)

        # Calculate importance as increase in loss
        importance = perturbed_loss - baseline_loss
        importances.append(importance.item())

    return dict(zip(feature_names, importances))

# =============================
# üéØ READY TO USE!
# =============================

print("\\nüéØ PIPELINE READY!")
print("Run: pipeline_results = run_complete_pipeline()")
print("Then use: pipeline_results['predict_function']([your_5_features])")

# Example usage:
if __name__ == "__main__":
    print("\\nüöÄ Ready to run complete analysis!")
    print("Execute: pipeline_results = run_complete_pipeline()")

# =============================
# üì¶ FINAL SETUP
# =============================

print("\n" + "="*60)
print("üéØ SETUP COMPLETE!")
print("="*60)

print("\nüöÄ Ready to proceed with your data!")
print("üìÅ Next step: Load your CSV files")

# =============================
# üöÄ COMPLETE GNN PIPELINE - READY TO USE
# =============================

print("üéØ COMPLETE GNN PIPELINE LOADED")
print("=" * 50)

# =============================
# üì¶ GLOBAL CONFIGURATION
# =============================

# Define features consistently
FEATURE_COLUMNS = ['Fruit_total_norm', 'speed_tot_norm', 'stem_tot_norm',
                   'stem_ht_norm', 'branch_cnt_norm']
TARGET_LOSS = 'fruit_loss_norm'
TARGET_OUTPUT = 'Total_fruit_norm'

# Global variables for consistency
scaler = None
loss_stats = None
output_stats = None

# =============================
# üìÅ STEP 1: DATA LOADING
# =============================

def load_data_files():
    """Upload and load your CSV files"""
    print("üìÅ UPLOAD YOUR DATA FILES")
    print("-" * 30)

    from google.colab import files
    uploaded = files.upload()

    # Identify files
    harvest_file = None
    adj_file = None

    for filename in uploaded.keys():
        if 'Norm' in filename and 'Adjacency' not in filename:
            harvest_file = filename
        elif 'Adjacency' in filename:
            adj_file = filename

    if harvest_file is None or adj_file is None:
        print("‚ùå Please upload both harvest data and adjacency matrix files")
        return None, None

    # Load files
    import io
    df = pd.read_csv(io.BytesIO(uploaded[harvest_file]))
    adj = pd.read_csv(io.BytesIO(uploaded[adj_file]), index_col=0)

    print(f"‚úÖ Loaded harvest data: {harvest_file} - {df.shape}")
    print(f"‚úÖ Loaded adjacency matrix: {adj_file} - {adj.shape}")

    return df, adj

def clean_and_sync_data(df, adj):
    """Clean and synchronize harvest data with adjacency matrix"""
    print("üßπ CLEANING AND SYNCHRONIZING DATA")
    print("-" * 40)

    # Clean harvest data
    original_shape = df.shape
    df = df.dropna()
    df = df[pd.notnull(df['row_id'])].copy()
    df['row_id'] = df['row_id'].round().astype(int)

    print(f"üìä Harvest data: {original_shape} ‚Üí {df.shape} (after cleaning)")

    # Clean adjacency matrix
    adj.index = adj.index.astype(int)
    adj.columns = adj.columns.astype(int)

    # Synchronize IDs
    harvest_ids = set(df['row_id'])
    adj_ids = set(adj.index)

    # Find common IDs
    common_ids = sorted(list(harvest_ids & adj_ids))
    missing_harvest = len(harvest_ids - adj_ids)
    missing_adj = len(adj_ids - harvest_ids)

    print(f"üìä Common IDs: {len(common_ids)}")
    print(f"üìä Missing in adjacency: {missing_harvest}")
    print(f"üìä Missing in harvest: {missing_adj}")

    # Keep only common IDs
    df_clean = df[df['row_id'].isin(common_ids)].copy()
    adj_clean = adj.loc[common_ids, common_ids]

    # Sort by row_id for consistency
    df_clean = df_clean.set_index('row_id').loc[common_ids].reset_index()

    print(f"‚úÖ Synchronized data: {df_clean.shape[0]} samples")

    return df_clean, adj_clean

# =============================
# üìä STEP 2: DATA PREPROCESSING
# =============================

def create_scalers_and_stats(df):
    """Create scalers and store statistics"""
    global scaler, loss_stats, output_stats

    print("üìä CREATING SCALERS AND STATISTICS")
    print("-" * 40)

    # Check if all features exist
    missing_features = [f for f in FEATURE_COLUMNS if f not in df.columns]
    if missing_features:
        print(f"‚ùå Missing features: {missing_features}")
        print(f"Available columns: {list(df.columns)}")
        return False

    # Create scaler
    scaler = StandardScaler()
    scaler.fit(df[FEATURE_COLUMNS])

    # Store target statistics
    loss_stats = {
        'mean': df[TARGET_LOSS].mean(),
        'std': df[TARGET_LOSS].std()
    }

    output_stats = {
        'mean': df[TARGET_OUTPUT].mean(),
        'std': df[TARGET_OUTPUT].std()
    }

    print(f"‚úÖ Scaler created for {len(FEATURE_COLUMNS)} features")
    print(f"‚úÖ Loss stats - Mean: {loss_stats['mean']:.4f}, Std: {loss_stats['std']:.4f}")
    print(f"‚úÖ Output stats - Mean: {output_stats['mean']:.4f}, Std: {output_stats['std']:.4f}")

    return True

def create_graph_data(df, adj):
    """Create PyTorch Geometric data object"""
    print("üîó CREATING GRAPH DATA OBJECT")
    print("-" * 40)

    # Scale features
    X_scaled = scaler.transform(df[FEATURE_COLUMNS])

    # Create tensors
    x = torch.tensor(X_scaled, dtype=torch.float32)
    y_loss = torch.tensor(df[TARGET_LOSS].values, dtype=torch.float32)
    y_output = torch.tensor(df[TARGET_OUTPUT].values, dtype=torch.float32)

    # Create edge index
    edge_indices = np.array(np.nonzero(adj.values))
    edge_index = torch.tensor(edge_indices, dtype=torch.long)

    # Create data object
    data = Data(x=x, edge_index=edge_index)
    data.y_loss = y_loss
    data.y_output = y_output

    print(f"‚úÖ Graph created:")
    print(f"   Nodes: {data.x.shape[0]}")
    print(f"   Edges: {data.edge_index.shape[1]}")
    print(f"   Features per node: {data.x.shape[1]}")
    print(f"   Self-loops: {(edge_index[0] == edge_index[1]).sum().item()}")

    return data

# =============================
# üß† STEP 3: MODEL DEFINITIONS
# =============================

class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, dropout=0.2):
        super().__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv3(x, edge_index)
        return x.squeeze()

class GraphSAGE(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, dropout=0.2):
        super().__init__()
        self.conv1 = SAGEConv(input_dim, hidden_dim)
        self.conv2 = SAGEConv(hidden_dim, hidden_dim)
        self.conv3 = SAGEConv(hidden_dim, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)

        x = self.conv3(x, edge_index)
        return x.squeeze()

# =============================
# üöÄ STEP 4: TRAINING FUNCTION
# =============================

def train_model(model, data, target_name, lr=0.01, epochs=500, verbose=True):
    """Train model with monitoring"""
    print(f"üöÄ Training {model.__class__.__name__} for {target_name}")

    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50)
    loss_fn = nn.MSELoss()

    target = getattr(data, target_name)

    best_loss = float('inf')
    losses = []

    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(data.x, data.edge_index)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
        scheduler.step(loss)

        losses.append(loss.item())

        if loss.item() < best_loss:
            best_loss = loss.item()

        if verbose and epoch % 100 == 0:
            print(f"  Epoch {epoch:3d} | Loss: {loss.item():.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}")

    # Final prediction
    model.eval()
    with torch.no_grad():
        final_output = model(data.x, data.edge_index)

    print(f"  ‚úÖ Training complete | Best Loss: {best_loss:.6f}")
    return final_output.detach(), losses

# =============================
# üìä STEP 5: EVALUATION
# =============================

def evaluate_model(model_name, target_name, y_true, y_pred):
    """Evaluate model performance"""
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)

    print(f"üìä {model_name:12s} | {target_name:15s} | R¬≤: {r2:.4f} | RMSE: {rmse:.6f}")

    return {
        'model': model_name,
        'target': target_name,
        'r2': r2,
        'rmse': rmse,
        'mse': mse
    }

# =============================
# üîÆ STEP 6: PREDICTION FUNCTIONS
# =============================

def predict_single_sample(model, raw_input, target_type='loss'):
    """Predict for single sample"""
    global scaler, loss_stats, output_stats

    # Validate input
    if len(raw_input) != len(FEATURE_COLUMNS):
        raise ValueError(f"Expected {len(FEATURE_COLUMNS)} features, got {len(raw_input)}")

    # Preprocess
    raw_input = np.array(raw_input).reshape(1, -1)
    scaled_input = scaler.transform(raw_input)
    x_tensor = torch.tensor(scaled_input, dtype=torch.float32)

    # Create single node graph
    edge_index = torch.tensor([[0], [0]], dtype=torch.long)

    # Predict
    model.eval()
    with torch.no_grad():
        pred = model(x_tensor, edge_index)

    # Denormalize
    if target_type == 'loss':
        pred_denorm = pred.item() * loss_stats['std'] + loss_stats['mean']
    else:
        pred_denorm = pred.item() * output_stats['std'] + output_stats['mean']

    return pred_denorm

# =============================
# üìà STEP 7: VISUALIZATION
# =============================

def plot_training_curves(losses_dict):
    """Plot training curves"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for i, (model_target, losses) in enumerate(losses_dict.items()):
        if i < 4:
            axes[i].plot(losses)
            axes[i].set_title(f"{model_target} Training Loss")
            axes[i].set_xlabel("Epoch")
            axes[i].set_ylabel("Loss")
            axes[i].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def plot_results_comparison(results_df):
    """Plot model comparison"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # R¬≤ comparison
    pivot_r2 = results_df.pivot(index="model", columns="target", values="r2")
    pivot_r2.plot(kind="bar", ax=ax1, color=['lightblue', 'lightcoral'])
    ax1.set_title("Model R¬≤ Comparison (Higher = Better)")
    ax1.set_ylabel("R¬≤ Score")
    ax1.set_ylim(0, 1)
    ax1.grid(True, alpha=0.3)
    ax1.legend(title="Target")
    ax1.tick_params(axis='x', rotation=45)

    # RMSE comparison
    pivot_rmse = results_df.pivot(index="model", columns="target", values="rmse")
    pivot_rmse.plot(kind="bar", ax=ax2, color=['orange', 'lightgreen'])
    ax2.set_title("Model RMSE Comparison (Lower = Better)")
    ax2.set_ylabel("RMSE")
    ax2.grid(True, alpha=0.3)
    ax2.legend(title="Target")
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

# =============================
# üöÄ STEP 8: MAIN PIPELINE
# =============================

def run_complete_pipeline():
    """Run the complete GNN pipeline"""
    print("üöÄ STARTING COMPLETE GNN PIPELINE")
    print("=" * 60)

    # Step 1: Load data
    df, adj = load_data_files()
    if df is None:
        return None

    # Step 2: Clean and sync
    df_clean, adj_clean = clean_and_sync_data(df, adj)

    # Step 3: Create scalers
    if not create_scalers_and_stats(df_clean):
        return None

    # Step 4: Create graph
    data = create_graph_data(df_clean, adj_clean)

    # Step 5: Train models
    print("\nüß† TRAINING MODELS")
    print("-" * 40)

    models = {}
    results = []
    losses_dict = {}

    # GCN Loss
    print("\nüîπ GCN for fruit_loss_norm")
    gcn_loss = GCN(input_dim=data.x.shape[1])
    gcn_pred_loss, gcn_loss_curve = train_model(gcn_loss, data, 'y_loss')
    models['gcn_loss'] = gcn_loss
    losses_dict['GCN_Loss'] = gcn_loss_curve
    results.append(evaluate_model("GCN", TARGET_LOSS, data.y_loss.numpy(), gcn_pred_loss.numpy()))

    # GCN Output
    print("\nüîπ GCN for Total_fruit_norm")
    gcn_output = GCN(input_dim=data.x.shape[1])
    gcn_pred_output, gcn_output_curve = train_model(gcn_output, data, 'y_output')
    models['gcn_output'] = gcn_output
    losses_dict['GCN_Output'] = gcn_output_curve
    results.append(evaluate_model("GCN", TARGET_OUTPUT, data.y_output.numpy(), gcn_pred_output.numpy()))

    # GraphSAGE Loss
    print("\nüîπ GraphSAGE for fruit_loss_norm")
    sage_loss = GraphSAGE(input_dim=data.x.shape[1])
    sage_pred_loss, sage_loss_curve = train_model(sage_loss, data, 'y_loss')
    models['sage_loss'] = sage_loss
    losses_dict['SAGE_Loss'] = sage_loss_curve
    results.append(evaluate_model("GraphSAGE", TARGET_LOSS, data.y_loss.numpy(), sage_pred_loss.numpy()))

    # GraphSAGE Output
    print("\nüîπ GraphSAGE for Total_fruit_norm")
    sage_output = GraphSAGE(input_dim=data.x.shape[1])
    sage_pred_output, sage_output_curve = train_model(sage_output, data, 'y_output')
    models['sage_output'] = sage_output
    losses_dict['SAGE_Output'] = sage_output_curve
    results.append(evaluate_model("GraphSAGE", TARGET_OUTPUT, data.y_output.numpy(), sage_pred_output.numpy()))

    # Step 6: Results summary
    print("\nüìä RESULTS SUMMARY")
    print("-" * 40)

    results_df = pd.DataFrame(results)
    print(results_df.round(4))

    # Step 7: Visualizations
    print("\nüìà CREATING VISUALIZATIONS")
    plot_training_curves(losses_dict)
    plot_results_comparison(results_df)

    # Step 8: Create prediction interface
    def predict_new_sample(raw_input):
        """Predict both targets for new input"""
        print(f"\nüîÆ MAKING PREDICTIONS")
        print(f"Input: {raw_input}")
        print("-" * 30)

        try:
            # Use best performing models (GraphSAGE typically)
            loss_pred = predict_single_sample(models['sage_loss'], raw_input, 'loss')
            output_pred = predict_single_sample(models['sage_output'], raw_input, 'output')

            print(f"üìâ Predicted {TARGET_LOSS}: {loss_pred:.6f}")
            print(f"üìà Predicted {TARGET_OUTPUT}: {output_pred:.6f}")

            return loss_pred, output_pred

        except Exception as e:
            print(f"‚ùå Prediction error: {e}")
            return None, None

    print("\n‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
    print("\nüéØ Ready for predictions!")
    print("Features required (in order):")
    for i, feature in enumerate(FEATURE_COLUMNS):
        print(f"  {i+1}. {feature}")

    return {
        'models': models,
        'data': data,
        'results': results_df,
        'predict_function': predict_new_sample,
        'feature_columns': FEATURE_COLUMNS
    }

# =============================
# üéØ READY TO USE!
# =============================

print("\nüéØ PIPELINE READY!")
print("Run: pipeline_results = run_complete_pipeline()")
print("Then use: pipeline_results['predict_function']([your_5_features])")

# =============================
# üöÄ RUN COMPLETE ANALYSIS
# =============================

print("üöÄ STARTING COMPLETE BLUEBERRY HARVEST ANALYSIS")
print("=" * 60)

# Step 1: Upload files and run pipeline
print("üìÅ Ready to upload your files...")
print("Please upload:")
print("  1. Harvest data file (containing 'Norm' in filename)")
print("  2. Adjacency matrix file (containing 'Adjacency' in filename)")
print()

# Run the complete pipeline
pipeline_results = run_complete_pipeline()

# =============================
# üîç ADDITIONAL ANALYSIS & COMPARISONS
# =============================

if pipeline_results is not None:
    print("\nüîç RUNNING ADDITIONAL ANALYSIS")
    print("=" * 50)

    # Extract results for detailed analysis
    models = pipeline_results['models']
    data = pipeline_results['data']
    results_df = pipeline_results['results']
    predict_function = pipeline_results['predict_function']

    # =============================
    # üìä DETAILED MODEL COMPARISON
    # =============================

    print("\nüìä DETAILED MODEL COMPARISON")
    print("-" * 40)

    # Create comprehensive results table
    print("\nüìà Complete Results Summary:")
    print(results_df.to_string(index=False))

    # Find best models
    loss_models = results_df[results_df['target'] == 'fruit_loss_norm']
    output_models = results_df[results_df['target'] == 'Total_fruit_norm']

    best_loss_model = loss_models.loc[loss_models['r2'].idxmax()]
    best_output_model = output_models.loc[output_models['r2'].idxmax()]

    print(f"\nüèÜ BEST MODELS:")
    print(f"  Best for fruit_loss_norm: {best_loss_model['model']} (R¬≤ = {best_loss_model['r2']:.4f})")
    print(f"  Best for Total_fruit_norm: {best_output_model['model']} (R¬≤ = {best_output_model['r2']:.4f})")

    # =============================
    # üìà PREDICTION VS ACTUAL PLOTS
    # =============================

    print("\nüìà Creating prediction vs actual plots...")

    def plot_predictions_vs_actual():
        """Create scatter plots of predictions vs actual"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # Get predictions from trained models
        models['gcn_loss'].eval()
        models['gcn_output'].eval()
        models['sage_loss'].eval()
        models['sage_output'].eval()

        with torch.no_grad():
            gcn_loss_pred = models['gcn_loss'](data.x, data.edge_index).numpy()
            gcn_output_pred = models['gcn_output'](data.x, data.edge_index).numpy()
            sage_loss_pred = models['sage_loss'](data.x, data.edge_index).numpy()
            sage_output_pred = models['sage_output'](data.x, data.edge_index).numpy()

        actual_loss = data.y_loss.numpy()
        actual_output = data.y_output.numpy()

        # Plot configurations
        plots_config = [
            (gcn_loss_pred, actual_loss, "GCN - Fruit Loss", axes[0,0]),
            (gcn_output_pred, actual_output, "GCN - Total Output", axes[0,1]),
            (sage_loss_pred, actual_loss, "GraphSAGE - Fruit Loss", axes[1,0]),
            (sage_output_pred, actual_output, "GraphSAGE - Total Output", axes[1,1])
        ]

        for pred, actual, title, ax in plots_config:
            # Scatter plot
            ax.scatter(actual, pred, alpha=0.6, s=20, color='blue')

            # Perfect prediction line
            min_val = min(actual.min(), pred.min())
            max_val = max(actual.max(), pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')

            # Calculate R¬≤
            r2 = r2_score(actual, pred)
            rmse = np.sqrt(mean_squared_error(actual, pred))

            ax.set_xlabel('Actual Values')
            ax.set_ylabel('Predicted Values')
            ax.set_title(f'{title}\nR¬≤ = {r2:.4f}, RMSE = {rmse:.4f}')
            ax.grid(True, alpha=0.3)
            ax.legend()

        plt.tight_layout()
        plt.show()

    plot_predictions_vs_actual()

    # =============================
    # üî¨ FEATURE IMPORTANCE ANALYSIS
    # =============================

    print("\nüî¨ FEATURE IMPORTANCE ANALYSIS")
    print("-" * 40)

    def analyze_feature_importance():
        """Analyze feature importance through permutation"""
        feature_names = pipeline_results['feature_columns']

        def get_feature_importance(model, target_data):
            model.eval()
            with torch.no_grad():
                baseline_pred = model(data.x, data.edge_index)
                baseline_mse = F.mse_loss(baseline_pred, target_data).item()

            importances = []
            for i in range(len(feature_names)):
                # Permute feature i
                perturbed_x = data.x.clone()
                perm_idx = torch.randperm(perturbed_x.size(0))
                perturbed_x[:, i] = perturbed_x[perm_idx, i]

                with torch.no_grad():
                    perturbed_pred = model(perturbed_x, data.edge_index)
                    perturbed_mse = F.mse_loss(perturbed_pred, target_data).item()

                importance = perturbed_mse - baseline_mse
                importances.append(importance)

            return importances

        # Get feature importance for best models
        if best_loss_model['model'] == 'GCN':
            loss_importance = get_feature_importance(models['gcn_loss'], data.y_loss)
        else:
            loss_importance = get_feature_importance(models['sage_loss'], data.y_loss)

        if best_output_model['model'] == 'GCN':
            output_importance = get_feature_importance(models['gcn_output'], data.y_output)
        else:
            output_importance = get_feature_importance(models['sage_output'], data.y_output)

        # Create importance DataFrame
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Loss_Importance': loss_importance,
            'Output_Importance': output_importance
        })

        print("üìä Feature Importance (Higher = More Important):")
        print(importance_df.round(6))

        # Plot feature importance
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        ax1.barh(feature_names, loss_importance, color='lightcoral')
        ax1.set_title('Feature Importance - Fruit Loss')
        ax1.set_xlabel('Importance Score')

        ax2.barh(feature_names, output_importance, color='lightblue')
        ax2.set_title('Feature Importance - Total Output')
        ax2.set_xlabel('Importance Score')

        plt.tight_layout()
        plt.show()

        return importance_df

    importance_results = analyze_feature_importance()

    # =============================
    # üéØ EXAMPLE PREDICTIONS
    # =============================

    print("\nüéØ EXAMPLE PREDICTIONS")
    print("-" * 40)

    # Get some sample data for examples
    sample_indices = [0, len(data.x)//4, len(data.x)//2, 3*len(data.x)//4, -1]

    print("üîÆ Sample Predictions:")
    print("(Comparing with actual values)")
    print()

    for i, idx in enumerate(sample_indices):
        if idx == -1:
            idx = len(data.x) - 1

        # Get actual values (need to denormalize)
        actual_loss = data.y_loss[idx].item()
        actual_output = data.y_output[idx].item()

        # Get input features (already scaled)
        sample_input = data.x[idx].numpy()

        # Make predictions
        try:
            pred_loss, pred_output = predict_function(sample_input)

            print(f"Sample {i+1}:")
            print(f"  Actual   - Loss: {actual_loss:.4f}, Output: {actual_output:.4f}")
            print(f"  Predicted - Loss: {pred_loss:.4f}, Output: {pred_output:.4f}")
            print(f"  Error    - Loss: {abs(pred_loss - actual_loss):.4f}, Output: {abs(pred_output - actual_output):.4f}")
            print()

        except Exception as e:
            print(f"  Error in prediction: {e}")

    # =============================
    # üìã COMPREHENSIVE SUMMARY
    # =============================

    print("\nüìã COMPREHENSIVE ANALYSIS SUMMARY")
    print("=" * 60)

    print("üéØ MODEL PERFORMANCE:")
    print(f"  Best Loss Model: {best_loss_model['model']} (R¬≤ = {best_loss_model['r2']:.4f})")
    print(f"  Best Output Model: {best_output_model['model']} (R¬≤ = {best_output_model['r2']:.4f})")

    print("\nüìä DATA STATISTICS:")
    print(f"  Total Samples: {len(data.x)}")
    print(f"  Features: {len(pipeline_results['feature_columns'])}")
    print(f"  Graph Edges: {data.edge_index.shape[1]}")
    print(f"  Graph Density: {data.edge_index.shape[1] / (len(data.x) * (len(data.x) - 1)):.4f}")

    print("\nüî¨ TOP IMPORTANT FEATURES:")
    loss_top = importance_results.nlargest(3, 'Loss_Importance')
    output_top = importance_results.nlargest(3, 'Output_Importance')

    print("  For Fruit Loss:")
    for _, row in loss_top.iterrows():
        print(f"    {row['Feature']}: {row['Loss_Importance']:.6f}")

    print("  For Total Output:")
    for _, row in output_top.iterrows():
        print(f"    {row['Feature']}: {row['Output_Importance']:.6f}")

    # =============================
    # üéÆ INTERACTIVE PREDICTION INTERFACE
    # =============================

    print("\nüéÆ INTERACTIVE PREDICTION INTERFACE")
    print("-" * 50)

    def interactive_predictions():
        """Interactive prediction interface"""
        print("Enter feature values for prediction:")
        print("(Features in order: " + ", ".join(pipeline_results['feature_columns']) + ")")

        while True:
            try:
                user_input = []
                for i, feature in enumerate(pipeline_results['feature_columns']):
                    value = float(input(f"{i+1}. {feature}: "))
                    user_input.append(value)

                loss_pred, output_pred = predict_function(user_input)

                print(f"\nüéØ PREDICTIONS:")
                print(f"  üìâ Fruit Loss: {loss_pred:.6f}")
                print(f"  üìà Total Output: {output_pred:.6f}")

                continue_pred = input("\nMake another prediction? (y/n): ").lower()
                if continue_pred != 'y':
                    break

            except ValueError:
                print("Please enter valid numbers.")
            except Exception as e:
                print(f"Error: {e}")
                break

    print("‚úÖ ANALYSIS COMPLETE!")
    print("\nüéØ To make interactive predictions, run:")
    print("interactive_predictions()")

    # Return comprehensive results
    return {
        'pipeline_results': pipeline_results,
        'best_models': {
            'loss': best_loss_model,
            'output': best_output_model
        },
        'feature_importance': importance_results,
        'interactive_function': interactive_predictions
    }

else:
    print("‚ùå Pipeline failed. Please check your files and try again.")
    return None

print("\n" + "="*60)
print("üèÅ COMPLETE ANALYSIS FINISHED!")
print("="*60)

# =============================
# üöÄ RUN COMPLETE ANALYSIS
# =============================

print("üöÄ STARTING COMPLETE BLUEBERRY HARVEST ANALYSIS")
print("=" * 60)

# Step 1: Upload files and run pipeline
print("üìÅ Ready to upload your files...")
print("Please upload:")
print("  1. Harvest data file (containing 'Norm' in filename)")
print("  2. Adjacency matrix file (containing 'Adjacency' in filename)")
print()

# Run the complete pipeline
pipeline_results = run_complete_pipeline()

# =============================
# üîç ADDITIONAL ANALYSIS & COMPARISONS
# =============================

def run_additional_analysis(pipeline_results):
    """Run additional analysis on pipeline results"""
    if pipeline_results is None:
        print("‚ùå No pipeline results to analyze")
        return None
    print("\nüîç RUNNING ADDITIONAL ANALYSIS")
    print("=" * 50)

    # Extract results for detailed analysis
    models = pipeline_results['models']
    data = pipeline_results['data']
    results_df = pipeline_results['results']
    predict_function = pipeline_results['predict_function']

    # =============================
    # üìä DETAILED MODEL COMPARISON
    # =============================

    print("\nüìä DETAILED MODEL COMPARISON")
    print("-" * 40)

    # Create comprehensive results table
    print("\nüìà Complete Results Summary:")
    print(results_df.to_string(index=False))

    # Find best models
    loss_models = results_df[results_df['target'] == 'fruit_loss_norm']
    output_models = results_df[results_df['target'] == 'Total_fruit_norm']

    best_loss_model = loss_models.loc[loss_models['r2'].idxmax()]
    best_output_model = output_models.loc[output_models['r2'].idxmax()]

    print(f"\nüèÜ BEST MODELS:")
    print(f"  Best for fruit_loss_norm: {best_loss_model['model']} (R¬≤ = {best_loss_model['r2']:.4f})")
    print(f"  Best for Total_fruit_norm: {best_output_model['model']} (R¬≤ = {best_output_model['r2']:.4f})")

    # =============================
    # üìà PREDICTION VS ACTUAL PLOTS
    # =============================

    print("\nüìà Creating prediction vs actual plots...")

    def plot_predictions_vs_actual():
        """Create scatter plots of predictions vs actual"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # Get predictions from trained models
        models['gcn_loss'].eval()
        models['gcn_output'].eval()
        models['sage_loss'].eval()
        models['sage_output'].eval()

        with torch.no_grad():
            gcn_loss_pred = models['gcn_loss'](data.x, data.edge_index).numpy()
            gcn_output_pred = models['gcn_output'](data.x, data.edge_index).numpy()
            sage_loss_pred = models['sage_loss'](data.x, data.edge_index).numpy()
            sage_output_pred = models['sage_output'](data.x, data.edge_index).numpy()

        actual_loss = data.y_loss.numpy()
        actual_output = data.y_output.numpy()

        # Plot configurations
        plots_config = [
            (gcn_loss_pred, actual_loss, "GCN - Fruit Loss", axes[0,0]),
            (gcn_output_pred, actual_output, "GCN - Total Output", axes[0,1]),
            (sage_loss_pred, actual_loss, "GraphSAGE - Fruit Loss", axes[1,0]),
            (sage_output_pred, actual_output, "GraphSAGE - Total Output", axes[1,1])
        ]

        for pred, actual, title, ax in plots_config:
            # Scatter plot
            ax.scatter(actual, pred, alpha=0.6, s=20, color='blue')

            # Perfect prediction line
            min_val = min(actual.min(), pred.min())
            max_val = max(actual.max(), pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')

            # Calculate R¬≤
            r2 = r2_score(actual, pred)
            rmse = np.sqrt(mean_squared_error(actual, pred))

            ax.set_xlabel('Actual Values')
            ax.set_ylabel('Predicted Values')
            ax.set_title(f'{title}\\nR¬≤ = {r2:.4f}, RMSE = {rmse:.4f}')
            ax.grid(True, alpha=0.3)
            ax.legend()

        plt.tight_layout()
        plt.show()

    plot_predictions_vs_actual()

    # =============================
    # üî¨ FEATURE IMPORTANCE ANALYSIS
    # =============================

    print("\nüî¨ FEATURE IMPORTANCE ANALYSIS")
    print("-" * 40)

    def analyze_feature_importance():
        """Analyze feature importance through permutation"""
        feature_names = pipeline_results['feature_columns']

        def get_feature_importance(model, target_data):
            model.eval()
            with torch.no_grad():
                baseline_pred = model(data.x, data.edge_index)
                baseline_mse = F.mse_loss(baseline_pred, target_data).item()

            importances = []
            for i in range(len(feature_names)):
                # Permute feature i
                perturbed_x = data.x.clone()
                perm_idx = torch.randperm(perturbed_x.size(0))
                perturbed_x[:, i] = perturbed_x[perm_idx, i]

                with torch.no_grad():
                    perturbed_pred = model(perturbed_x, data.edge_index)
                    perturbed_mse = F.mse_loss(perturbed_pred, target_data).item()

                importance = perturbed_mse - baseline_mse
                importances.append(importance)

            return importances

        # Get feature importance for best models
        if best_loss_model['model'] == 'GCN':
            loss_importance = get_feature_importance(models['gcn_loss'], data.y_loss)
        else:
            loss_importance = get_feature_importance(models['sage_loss'], data.y_loss)

        if best_output_model['model'] == 'GCN':
            output_importance = get_feature_importance(models['gcn_output'], data.y_output)
        else:
            output_importance = get_feature_importance(models['sage_output'], data.y_output)

        # Create importance DataFrame
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Loss_Importance': loss_importance,
            'Output_Importance': output_importance
        })

        print("üìä Feature Importance (Higher = More Important):")
        print(importance_df.round(6))

        # Plot feature importance
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        ax1.barh(feature_names, loss_importance, color='lightcoral')
        ax1.set_title('Feature Importance - Fruit Loss')
        ax1.set_xlabel('Importance Score')

        ax2.barh(feature_names, output_importance, color='lightblue')
        ax2.set_title('Feature Importance - Total Output')
        ax2.set_xlabel('Importance Score')

        plt.tight_layout()
        plt.show()

        return importance_df

    importance_results = analyze_feature_importance()

    # =============================
    # üéØ EXAMPLE PREDICTIONS
    # =============================

    print("\nüéØ EXAMPLE PREDICTIONS")
    print("-" * 40)

    # Get some sample data for examples
    sample_indices = [0, len(data.x)//4, len(data.x)//2, 3*len(data.x)//4, -1]

    print("üîÆ Sample Predictions:")
    print("(Comparing with actual values)")
    print()

    for i, idx in enumerate(sample_indices):
        if idx == -1:
            idx = len(data.x) - 1

        # Get actual values (need to denormalize)
        actual_loss = data.y_loss[idx].item()
        actual_output = data.y_output[idx].item()

        # Get input features (already scaled)
        sample_input = data.x[idx].numpy()

        # Make predictions
        try:
            pred_loss, pred_output = predict_function(sample_input)

            print(f"Sample {i+1}:")
            print(f"  Actual   - Loss: {actual_loss:.4f}, Output: {actual_output:.4f}")
            print(f"  Predicted - Loss: {pred_loss:.4f}, Output: {pred_output:.4f}")
            print(f"  Error    - Loss: {abs(actual_loss - pred_loss):.4f}, Output: {abs(actual_output - pred_output):.4f}")
            print()

        except Exception as e:
            print(f"  Error in prediction: {e}")

    # =============================
    # üìã COMPREHENSIVE SUMMARY
    # =============================

    print("\nüìã COMPREHENSIVE ANALYSIS SUMMARY")
    print("=" * 60)

    print("üéØ MODEL PERFORMANCE:")
    print(f"  Best Loss Model: {best_loss_model['model']} (R¬≤ = {best_loss_model['r2']:.4f})")
    print(f"  Best Output Model: {best_output_model['model']} (R¬≤ = {best_output_model['r2']:.4f})")

    print("\nüìä DATA STATISTICS:")
    print(f"  Total Samples: {len(data.x)}")
    print(f"  Features: {len(pipeline_results['feature_columns'])}")
    print(f"  Graph Edges: {data.edge_index.shape[1]}")
    print(f"  Graph Density: {data.edge_index.shape[1] / (len(data.x) * (len(data.x) - 1)):.4f}")

    print("\nüî¨ TOP IMPORTANT FEATURES:")
    loss_top = importance_results.nlargest(3, 'Loss_Importance')
    output_top = importance_results.nlargest(3, 'Output_Importance')

    print("  For Fruit Loss:")
    for _, row in loss_top.iterrows():
        print(f"    {row['Feature']}: {row['Loss_Importance']:.6f}")

    print("  For Total Output:")
    for _, row in output_top.iterrows():
        print(f"    {row['Feature']}: {row['Output_Importance']:.6f}")

    # =============================
    # üéÆ INTERACTIVE PREDICTION INTERFACE
    # =============================

    print("\nüéÆ INTERACTIVE PREDICTION INTERFACE")
    print("-" * 50)

    def interactive_predictions():
        """Interactive prediction interface"""
        print("Enter feature values for prediction:")
        print("(Features in order: " + ", ".join(pipeline_results['feature_columns']) + ")")

        while True:
            try:
                user_input = []
                for i, feature in enumerate(pipeline_results['feature_columns']):
                    value = float(input(f"{i+1}. {feature}: "))
                    user_input.append(value)

                loss_pred, output_pred = predict_function(user_input)

                print(f"\nüéØ PREDICTIONS:")
                print(f"  üìâ Fruit Loss: {loss_pred:.6f}")
                print(f"  üìà Total Output: {output_pred:.6f}")

                continue_pred = input("\nMake another prediction? (y/n): ").lower()
                if continue_pred != 'y':
                    break

            except ValueError:
                print("Please enter valid numbers.")
            except Exception as e:
                print(f"Error: {e}")
                break

    print("‚úÖ ANALYSIS COMPLETE!")
    print("\nüéØ To make interactive predictions, run:")
    print("interactive_predictions()")

    # Return comprehensive results
    return {
        'pipeline_results': pipeline_results,
        'best_models': {
            'loss': best_loss_model,
            'output': best_output_model
        },
        'feature_importance': importance_results,
        'interactive_function': interactive_predictions
    }

# =============================
# üöÄ MAIN EXECUTION
# =============================

# Run the complete pipeline
pipeline_results = run_complete_pipeline()

if pipeline_results is not None:
    print("\nüîç RUNNING ADDITIONAL ANALYSIS...")
    analysis_results = run_additional_analysis(pipeline_results)

    if analysis_results:
        print("\n‚úÖ COMPLETE ANALYSIS FINISHED!")
        print("üéØ Use analysis_results['interactive_function']() for interactive predictions")
else:
    print("‚ùå Pipeline failed. Please check your files and try again.")
    analysis_results = None

print("\n" + "="*60)
print("üèÅ COMPLETE ANALYSIS FINISHED!")
print("="*60)



# =============================
# üì¶ INSTALL NETWORKX - COMPLETE SETUP
# =============================

print("üì¶ Installing NetworkX for graph visualization...")

# Method 1: Standard pip install
try:
    !pip install networkx --quiet
    print("‚úÖ NetworkX installation attempt 1 completed")
except Exception as e:
    print(f"‚ö†Ô∏è Method 1 failed: {e}")

# Method 2: Force reinstall if needed
try:
    !pip install --upgrade --force-reinstall networkx --quiet
    print("‚úÖ NetworkX force reinstall completed")
except Exception as e:
    print(f"‚ö†Ô∏è Method 2 failed: {e}")

# Method 3: Install with specific version
try:
    !pip install networkx==3.1 --quiet
    print("‚úÖ NetworkX specific version installed")
except Exception as e:
    print(f"‚ö†Ô∏è Method 3 failed: {e}")

# Test the installation
print("\nüîç Testing NetworkX installation...")

try:
    import networkx as nx
    print(f"‚úÖ NetworkX imported successfully!")
    print(f"   Version: {nx.__version__}")

    # Test basic functionality
    G = nx.Graph()
    G.add_edge(0, 1)
    G.add_edge(1, 2)
    print(f"‚úÖ NetworkX basic functionality works!")
    print(f"   Test graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

    # Test layout algorithms
    pos = nx.spring_layout(G)
    print(f"‚úÖ NetworkX layout algorithms work!")

    # Test graph metrics
    degrees = dict(G.degree())
    clustering = nx.clustering(G)
    print(f"‚úÖ NetworkX graph metrics work!")

    print("\nüéâ NETWORKX SUCCESSFULLY INSTALLED AND TESTED!")

except ImportError as e:
    print(f"‚ùå NetworkX import failed: {e}")
    print("\nüîß Trying alternative installation methods...")

    # Alternative method using conda (if available)
    try:
        !conda install -c conda-forge networkx -y --quiet
        import networkx as nx
        print("‚úÖ NetworkX installed via conda!")
    except:
        print("‚ùå Conda installation also failed")

        # Last resort: install from GitHub
        try:
            !pip install git+https://github.com/networkx/networkx.git --quiet
            import networkx as nx
            print("‚úÖ NetworkX installed from GitHub!")
        except:
            print("‚ùå All installation methods failed")
            print("üîÑ Please try manually: !pip install networkx")

except Exception as e:
    print(f"‚ùå NetworkX test failed: {e}")

# Verify final installation
print("\nüéØ FINAL VERIFICATION:")
try:
    import networkx as nx
    print("‚úÖ NetworkX is ready to use!")
    print("üöÄ You can now run the complete graph analysis!")

    # Quick demo
    demo_graph = nx.karate_club_graph()
    print(f"üìä Demo: Karate Club graph has {demo_graph.number_of_nodes()} nodes")

except:
    print("‚ùå NetworkX not available")
    print("üí° You can still use the simple graph analysis without NetworkX")

print("\n" + "="*50)
print("üéØ INSTALLATION COMPLETE!")
print("="*50)

# =============================
# üöÄ RUN COMPLETE ANALYSIS
# =============================

print("üöÄ STARTING COMPLETE BLUEBERRY HARVEST ANALYSIS")
print("=" * 60)

# Step 1: Upload files and run pipeline
print("üìÅ Ready to upload your files...")
print("Please upload:")
print("  1. Harvest data file (containing 'Norm' in filename)")
print("  2. Adjacency matrix file (containing 'Adjacency' in filename)")
print()

# Run the complete pipeline
pipeline_results = run_complete_pipeline()

# =============================
# üîç ADDITIONAL ANALYSIS & COMPARISONS
# =============================

def run_additional_analysis(pipeline_results):
    """Run additional analysis on pipeline results"""
    if pipeline_results is None:
        print("‚ùå No pipeline results to analyze")
        return None
    print("\nüîç RUNNING ADDITIONAL ANALYSIS")
    print("=" * 50)

    # Extract results for detailed analysis
    models = pipeline_results['models']
    data = pipeline_results['data']
    results_df = pipeline_results['results']
    predict_function = pipeline_results['predict_function']

    # =============================
    # üìä DETAILED MODEL COMPARISON
    # =============================

    print("\nüìä DETAILED MODEL COMPARISON")
    print("-" * 40)

    # Create comprehensive results table
    print("\nüìà Complete Results Summary:")
    print(results_df.to_string(index=False))

    # Find best models
    loss_models = results_df[results_df['target'] == 'fruit_loss_norm']
    output_models = results_df[results_df['target'] == 'Total_fruit_norm']

    best_loss_model = loss_models.loc[loss_models['r2'].idxmax()]
    best_output_model = output_models.loc[output_models['r2'].idxmax()]

    print(f"\nüèÜ BEST MODELS:")
    print(f"  Best for fruit_loss_norm: {best_loss_model['model']} (R¬≤ = {best_loss_model['r2']:.4f})")
    print(f"  Best for Total_fruit_norm: {best_output_model['model']} (R¬≤ = {best_output_model['r2']:.4f})")

    # =============================
    # üìà PREDICTION VS ACTUAL PLOTS
    # =============================

    print("\nüìà Creating prediction vs actual plots...")

    def plot_predictions_vs_actual():
        """Create scatter plots of predictions vs actual"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # Get predictions from trained models
        models['gcn_loss'].eval()
        models['gcn_output'].eval()
        models['sage_loss'].eval()
        models['sage_output'].eval()

        with torch.no_grad():
            gcn_loss_pred = models['gcn_loss'](data.x, data.edge_index).numpy()
            gcn_output_pred = models['gcn_output'](data.x, data.edge_index).numpy()
            sage_loss_pred = models['sage_loss'](data.x, data.edge_index).numpy()
            sage_output_pred = models['sage_output'](data.x, data.edge_index).numpy()

        actual_loss = data.y_loss.numpy()
        actual_output = data.y_output.numpy()

        # Plot configurations
        plots_config = [
            (gcn_loss_pred, actual_loss, "GCN - Fruit Loss", axes[0,0]),
            (gcn_output_pred, actual_output, "GCN - Total Output", axes[0,1]),
            (sage_loss_pred, actual_loss, "GraphSAGE - Fruit Loss", axes[1,0]),
            (sage_output_pred, actual_output, "GraphSAGE - Total Output", axes[1,1])
        ]

        for pred, actual, title, ax in plots_config:
            # Scatter plot
            ax.scatter(actual, pred, alpha=0.6, s=20, color='blue')

            # Perfect prediction line
            min_val = min(actual.min(), pred.min())
            max_val = max(actual.max(), pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')

            # Calculate R¬≤
            r2 = r2_score(actual, pred)
            rmse = np.sqrt(mean_squared_error(actual, pred))

            ax.set_xlabel('Actual Values')
            ax.set_ylabel('Predicted Values')
            ax.set_title(f'{title}\\nR¬≤ = {r2:.4f}, RMSE = {rmse:.4f}')
            ax.grid(True, alpha=0.3)
            ax.legend()

        plt.tight_layout()
        plt.show()

    plot_predictions_vs_actual()

    # =============================
    # üîó GRAPH NETWORK VISUALIZATION
    # =============================

    print("\nüîó CREATING GRAPH NETWORK VISUALIZATIONS")
    print("-" * 40)

    def visualize_graph_network():
        """Visualize the graph network based on adjacency matrix"""
        import networkx as nx
        from matplotlib.patches import Patch

        # Convert PyTorch Geometric data to NetworkX
        edge_list = data.edge_index.t().numpy()
        G = nx.Graph()

        # Add nodes
        for i in range(data.x.shape[0]):
            G.add_node(i)

        # Add edges
        for edge in edge_list:
            if edge[0] != edge[1]:  # Skip self-loops for cleaner visualization
                G.add_edge(edge[0], edge[1])

        # Calculate node properties for visualization
        node_degrees = dict(G.degree())
        max_degree = max(node_degrees.values()) if node_degrees else 1

        # Get actual target values for color coding
        loss_values = data.y_loss.numpy()
        output_values = data.y_output.numpy()

        # Create subplots for different visualizations
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))

        # Layout for consistent positioning
        if len(G.nodes()) <= 100:
            pos = nx.spring_layout(G, k=1, iterations=50, seed=42)
        else:
            # For larger graphs, use faster layout
            pos = nx.spring_layout(G, k=0.5, iterations=30, seed=42)

        # 1. Basic Graph Structure
        ax1 = axes[0, 0]
        node_sizes = [300 + (node_degrees.get(node, 0) / max_degree) * 200 for node in G.nodes()]

        nx.draw_networkx_nodes(G, pos, node_size=node_sizes,
                              node_color='lightblue', alpha=0.7, ax=ax1)
        nx.draw_networkx_edges(G, pos, alpha=0.3, width=0.5, ax=ax1)

        ax1.set_title(f"Graph Structure\\nNodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}")
        ax1.axis('off')

        # 2. Nodes colored by Fruit Loss
        ax2 = axes[0, 1]
        nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes,
                                      node_color=loss_values, cmap='Reds',
                                      alpha=0.8, ax=ax2)
        nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5, ax=ax2)

        plt.colorbar(nodes, ax=ax2, label='Fruit Loss (normalized)')
        ax2.set_title("Nodes Colored by Fruit Loss")
        ax2.axis('off')

        # 3. Nodes colored by Total Output
        ax3 = axes[1, 0]
        nodes2 = nx.draw_networkx_nodes(G, pos, node_size=node_sizes,
                                       node_color=output_values, cmap='Greens',
                                       alpha=0.8, ax=ax3)
        nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5, ax=ax3)

        plt.colorbar(nodes2, ax=ax3, label='Total Output (normalized)')
        ax3.set_title("Nodes Colored by Total Output")
        ax3.axis('off')

        # 4. Degree Distribution and Statistics
        ax4 = axes[1, 1]
        degrees = list(node_degrees.values())
        ax4.hist(degrees, bins=min(20, max(degrees)), alpha=0.7, color='skyblue', edgecolor='black')
        ax4.set_xlabel('Node Degree')
        ax4.set_ylabel('Frequency')
        ax4.set_title('Degree Distribution')
        ax4.grid(True, alpha=0.3)

        # Add statistics text
        stats_text = f"""Graph Statistics:
        Nodes: {G.number_of_nodes()}
        Edges: {G.number_of_edges()}
        Avg Degree: {np.mean(degrees):.2f}
        Max Degree: {max(degrees)}
        Density: {nx.density(G):.4f}
        Connected: {nx.is_connected(G)}"""

        ax4.text(0.02, 0.98, stats_text, transform=ax4.transAxes,
                verticalalignment='top', fontsize=10,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

        plt.tight_layout()
        plt.show()

        return G, pos

    def visualize_node_neighborhoods():
        """Visualize specific node neighborhoods"""
        # Convert to NetworkX for easier neighborhood analysis
        edge_list = data.edge_index.t().numpy()
        G = nx.Graph()
        for i in range(data.x.shape[0]):
            G.add_node(i)
        for edge in edge_list:
            if edge[0] != edge[1]:
                G.add_edge(edge[0], edge[1])

        # Find nodes with highest and lowest target values
        loss_values = data.y_loss.numpy()
        output_values = data.y_output.numpy()

        # Get indices of extreme values
        high_loss_idx = np.argmax(loss_values)
        low_loss_idx = np.argmin(loss_values)
        high_output_idx = np.argmax(output_values)
        low_output_idx = np.argmin(output_values)

        extreme_nodes = [high_loss_idx, low_loss_idx, high_output_idx, low_output_idx]
        titles = ["Highest Loss Node", "Lowest Loss Node", "Highest Output Node", "Lowest Output Node"]

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()

        for i, (node_idx, title) in enumerate(zip(extreme_nodes, titles)):
            ax = axes[i]

            # Get neighborhood
            neighbors = list(G.neighbors(node_idx))
            neighborhood_nodes = [node_idx] + neighbors

            # Create subgraph
            subG = G.subgraph(neighborhood_nodes)
            pos = nx.spring_layout(subG, seed=42)

            # Color nodes
            node_colors = []
            for node in subG.nodes():
                if node == node_idx:
                    node_colors.append('red')  # Central node
                else:
                    node_colors.append('lightblue')  # Neighbors

            # Draw
            nx.draw_networkx_nodes(subG, pos, node_color=node_colors,
                                  node_size=500, alpha=0.8, ax=ax)
            nx.draw_networkx_edges(subG, pos, alpha=0.6, ax=ax)
            nx.draw_networkx_labels(subG, pos, font_size=8, ax=ax)

            # Add info
            loss_val = loss_values[node_idx]
            output_val = output_values[node_idx]
            degree = G.degree(node_idx)

            info_text = f"Node {node_idx}\\nLoss: {loss_val:.4f}\\nOutput: {output_val:.4f}\\nDegree: {degree}"
            ax.text(0.02, 0.98, info_text, transform=ax.transAxes,
                   verticalalignment='top', fontsize=9,
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

            ax.set_title(title)
            ax.axis('off')

        plt.tight_layout()
        plt.show()

    def analyze_graph_properties():
        """Analyze graph properties and their relationship to targets"""
        # Convert to NetworkX
        edge_list = data.edge_index.t().numpy()
        G = nx.Graph()
        for i in range(data.x.shape[0]):
            G.add_node(i)
        for edge in edge_list:
            if edge[0] != edge[1]:
                G.add_edge(edge[0], edge[1])

        # Calculate graph properties for each node
        degrees = dict(G.degree())
        clustering = nx.clustering(G)

        try:
            betweenness = nx.betweenness_centrality(G)
            closeness = nx.closeness_centrality(G)
        except:
            # For disconnected graphs
            betweenness = {node: 0 for node in G.nodes()}
            closeness = {node: 0 for node in G.nodes()}

        # Create DataFrame with node properties
        node_properties = pd.DataFrame({
            'node_id': list(G.nodes()),
            'degree': [degrees[i] for i in G.nodes()],
            'clustering': [clustering[i] for i in G.nodes()],
            'betweenness': [betweenness[i] for i in G.nodes()],
            'closeness': [closeness[i] for i in G.nodes()],
            'fruit_loss': data.y_loss.numpy(),
            'total_output': data.y_output.numpy()
        })

        # Correlation analysis
        print("\\nüîç Graph Properties vs Target Variables:")
        correlations = node_properties[['degree', 'clustering', 'betweenness', 'closeness',
                                      'fruit_loss', 'total_output']].corr()

        print("\\nCorrelation Matrix:")
        print(correlations.round(4))

        # Plot correlations
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # Degree vs targets
        axes[0,0].scatter(node_properties['degree'], node_properties['fruit_loss'], alpha=0.6, color='red')
        axes[0,0].set_xlabel('Node Degree')
        axes[0,0].set_ylabel('Fruit Loss')
        axes[0,0].set_title(f"Degree vs Fruit Loss\\n(Correlation: {correlations.loc['degree', 'fruit_loss']:.3f})")
        axes[0,0].grid(True, alpha=0.3)

        axes[0,1].scatter(node_properties['degree'], node_properties['total_output'], alpha=0.6, color='green')
        axes[0,1].set_xlabel('Node Degree')
        axes[0,1].set_ylabel('Total Output')
        axes[0,1].set_title(f"Degree vs Total Output\\n(Correlation: {correlations.loc['degree', 'total_output']:.3f})")
        axes[0,1].grid(True, alpha=0.3)

        # Clustering vs targets
        axes[1,0].scatter(node_properties['clustering'], node_properties['fruit_loss'], alpha=0.6, color='orange')
        axes[1,0].set_xlabel('Clustering Coefficient')
        axes[1,0].set_ylabel('Fruit Loss')
        axes[1,0].set_title(f"Clustering vs Fruit Loss\\n(Correlation: {correlations.loc['clustering', 'fruit_loss']:.3f})")
        axes[1,0].grid(True, alpha=0.3)

        axes[1,1].scatter(node_properties['clustering'], node_properties['total_output'], alpha=0.6, color='purple')
        axes[1,1].set_xlabel('Clustering Coefficient')
        axes[1,1].set_ylabel('Total Output')
        axes[1,1].set_title(f"Clustering vs Total Output\\n(Correlation: {correlations.loc['clustering', 'total_output']:.3f})")
        axes[1,1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        return node_properties, correlations

    def create_interactive_graph_explorer():
        """Create an interactive graph exploration function"""
        def explore_node(node_id):
            """Explore a specific node and its neighborhood"""
            if node_id >= len(data.x) or node_id < 0:
                print(f"‚ùå Invalid node ID. Must be between 0 and {len(data.x)-1}")
                return

            # Get node information
            print(f"\\nüîç EXPLORING NODE {node_id}")
            print("-" * 30)

            # Features
            features = data.x[node_id].numpy()
            print("üìä Features:")
            for i, (feature_name, value) in enumerate(zip(pipeline_results['feature_columns'], features)):
                print(f"  {feature_name}: {value:.4f}")

            # Targets
            loss_val = data.y_loss[node_id].item()
            output_val = data.y_output[node_id].item()
            print(f"\\nüéØ Targets:")
            print(f"  Fruit Loss: {loss_val:.4f}")
            print(f"  Total Output: {output_val:.4f}")

            # Graph properties
            edge_list = data.edge_index.t().numpy()
            neighbors = []
            for edge in edge_list:
                if edge[0] == node_id and edge[1] != node_id:
                    neighbors.append(edge[1])
                elif edge[1] == node_id and edge[0] != node_id:
                    neighbors.append(edge[0])

            neighbors = list(set(neighbors))
            print(f"\\nüîó Graph Properties:")
            print(f"  Degree: {len(neighbors)}")
            print(f"  Neighbors: {neighbors[:10]}{'...' if len(neighbors) > 10 else ''}")

            # Make prediction for this node
            try:
                pred_loss, pred_output = predict_function(features)
                print(f"\\nüîÆ Model Predictions:")
                print(f"  Predicted Loss: {pred_loss:.4f} (Actual: {loss_val:.4f})")
                print(f"  Predicted Output: {pred_output:.4f} (Actual: {output_val:.4f})")
                print(f"  Loss Error: {abs(pred_loss - loss_val):.4f}")
                print(f"  Output Error: {abs(pred_output - output_val):.4f}")
            except Exception as e:
                print(f"  ‚ùå Prediction error: {e}")

        return explore_node

    # Run graph visualizations
    print("üîó Creating main graph visualization...")
    G, pos = visualize_graph_network()

    print("\\nüîç Analyzing node neighborhoods...")
    visualize_node_neighborhoods()

    print("\\nüìä Analyzing graph properties...")
    node_properties, correlations = analyze_graph_properties()

    print("\\nüéÆ Creating interactive graph explorer...")
    explore_node = create_interactive_graph_explorer()

    print("\\n‚úÖ Graph visualizations complete!")
    print("üéØ Use explore_node(node_id) to explore specific nodes")

    return G, node_properties, correlations, explore_node

    # =============================
    # üî¨ FEATURE IMPORTANCE ANALYSIS
    # =============================

    print("\nüî¨ FEATURE IMPORTANCE ANALYSIS")
    print("-" * 40)

    def analyze_feature_importance():
        """Analyze feature importance through permutation"""
        feature_names = pipeline_results['feature_columns']

        def get_feature_importance(model, target_data):
            model.eval()
            with torch.no_grad():
                baseline_pred = model(data.x, data.edge_index)
                baseline_mse = F.mse_loss(baseline_pred, target_data).item()

            importances = []
            for i in range(len(feature_names)):
                # Permute feature i
                perturbed_x = data.x.clone()
                perm_idx = torch.randperm(perturbed_x.size(0))
                perturbed_x[:, i] = perturbed_x[perm_idx, i]

                with torch.no_grad():
                    perturbed_pred = model(perturbed_x, data.edge_index)
                    perturbed_mse = F.mse_loss(perturbed_pred, target_data).item()

                importance = perturbed_mse - baseline_mse
                importances.append(importance)

            return importances

        # Get feature importance for best models
        if best_loss_model['model'] == 'GCN':
            loss_importance = get_feature_importance(models['gcn_loss'], data.y_loss)
        else:
            loss_importance = get_feature_importance(models['sage_loss'], data.y_loss)

        if best_output_model['model'] == 'GCN':
            output_importance = get_feature_importance(models['gcn_output'], data.y_output)
        else:
            output_importance = get_feature_importance(models['sage_output'], data.y_output)

        # Create importance DataFrame
        importance_df = pd.DataFrame({
            'Feature': feature_names,
            'Loss_Importance': loss_importance,
            'Output_Importance': output_importance
        })

        print("üìä Feature Importance (Higher = More Important):")
        print(importance_df.round(6))

        # Plot feature importance
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        ax1.barh(feature_names, loss_importance, color='lightcoral')
        ax1.set_title('Feature Importance - Fruit Loss')
        ax1.set_xlabel('Importance Score')

        ax2.barh(feature_names, output_importance, color='lightblue')
        ax2.set_title('Feature Importance - Total Output')
        ax2.set_xlabel('Importance Score')

        plt.tight_layout()
        plt.show()

        return importance_df

    importance_results = analyze_feature_importance()

    # =============================
    # üéØ EXAMPLE PREDICTIONS
    # =============================

    print("\nüéØ EXAMPLE PREDICTIONS")
    print("-" * 40)

    # Get some sample data for examples
    sample_indices = [0, len(data.x)//4, len(data.x)//2, 3*len(data.x)//4, -1]

    print("üîÆ Sample Predictions:")
    print("(Comparing with actual values)")
    print()

    for i, idx in enumerate(sample_indices):
        if idx == -1:
            idx = len(data.x) - 1

        # Get actual values (need to denormalize)
        actual_loss = data.y_loss[idx].item()
        actual_output = data.y_output[idx].item()

        # Get input features (already scaled)
        sample_input = data.x[idx].numpy()

        # Make predictions
        try:
            pred_loss, pred_output = predict_function(sample_input)

            print(f"Sample {i+1}:")
            print(f"  Actual   - Loss: {actual_loss:.4f}, Output: {actual_output:.4f}")
            print(f"  Predicted - Loss: {pred_loss:.4f}, Output: {pred_output:.4f}")
            print(f"  Error    - Loss: {abs(actual_loss - pred_loss):.4f}, Output: {abs(actual_output - pred_output):.4f}")
            print()

        except Exception as e:
            print(f"  Error in prediction: {e}")

    # =============================
    # üìã COMPREHENSIVE SUMMARY
    # =============================

    print("\nüìã COMPREHENSIVE ANALYSIS SUMMARY")
    print("=" * 60)

    print("üéØ MODEL PERFORMANCE:")
    print(f"  Best Loss Model: {best_loss_model['model']} (R¬≤ = {best_loss_model['r2']:.4f})")
    print(f"  Best Output Model: {best_output_model['model']} (R¬≤ = {best_output_model['r2']:.4f})")

    print("\nüìä DATA STATISTICS:")
    print(f"  Total Samples: {len(data.x)}")
    print(f"  Features: {len(pipeline_results['feature_columns'])}")
    print(f"  Graph Edges: {data.edge_index.shape[1]}")
    print(f"  Graph Density: {data.edge_index.shape[1] / (len(data.x) * (len(data.x) - 1)):.4f}")

    print("\nüî¨ TOP IMPORTANT FEATURES:")
    loss_top = importance_results.nlargest(3, 'Loss_Importance')
    output_top = importance_results.nlargest(3, 'Output_Importance')

    print("  For Fruit Loss:")
    for _, row in loss_top.iterrows():
        print(f"    {row['Feature']}: {row['Loss_Importance']:.6f}")

    print("  For Total Output:")
    for _, row in output_top.iterrows():
        print(f"    {row['Feature']}: {row['Output_Importance']:.6f}")

    # =============================
    # üéÆ INTERACTIVE PREDICTION INTERFACE
    # =============================

    print("\nüéÆ INTERACTIVE PREDICTION INTERFACE")
    print("-" * 50)

    def interactive_predictions():
        """Interactive prediction interface"""
        print("Enter feature values for prediction:")
        print("(Features in order: " + ", ".join(pipeline_results['feature_columns']) + ")")

        while True:
            try:
                user_input = []
                for i, feature in enumerate(pipeline_results['feature_columns']):
                    value = float(input(f"{i+1}. {feature}: "))
                    user_input.append(value)

                loss_pred, output_pred = predict_function(user_input)

                print(f"\nüéØ PREDICTIONS:")
                print(f"  üìâ Fruit Loss: {loss_pred:.6f}")
                print(f"  üìà Total Output: {output_pred:.6f}")

                continue_pred = input("\nMake another prediction? (y/n): ").lower()
                if continue_pred != 'y':
                    break

            except ValueError:
                print("Please enter valid numbers.")
            except Exception as e:
                print(f"Error: {e}")
                break

    print("‚úÖ ANALYSIS COMPLETE!")
    print("\nüéØ To make interactive predictions, run:")
    print("interactive_predictions()")

    # Return comprehensive results
    return {
        'pipeline_results': pipeline_results,
        'best_models': {
            'loss': best_loss_model,
            'output': best_output_model
        },
        'feature_importance': importance_results,
        'interactive_function': interactive_predictions,
        'graph_network': G,
        'node_properties': node_properties,
        'graph_correlations': correlations,
        'node_explorer': explore_node
    }

# =============================
# üöÄ MAIN EXECUTION
# =============================

# Run the complete pipeline
pipeline_results = run_complete_pipeline()

if pipeline_results is not None:
    print("\nüîç RUNNING ADDITIONAL ANALYSIS...")
    analysis_results = run_additional_analysis(pipeline_results)

    if analysis_results:
        print("\n‚úÖ COMPLETE ANALYSIS FINISHED!")
        print("üéØ Use analysis_results['interactive_function']() for interactive predictions")
else:
    print("‚ùå Pipeline failed. Please check your files and try again.")
    analysis_results = None

print("\n" + "="*60)
print("üèÅ COMPLETE ANALYSIS FINISHED!")
print("="*60)